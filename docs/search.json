[
  {
    "objectID": "dataanalysis-exercise/results/readme.html",
    "href": "dataanalysis-exercise/results/readme.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "dataanalysis-exercise/data/readme.html",
    "href": "dataanalysis-exercise/data/readme.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Bio\n\n\n\n\n\nI am a second year part-time PhD student in Epidemiology at UGA. I have spent over five years working in sub-Saharan Africa on both HIV and malaria programs at the health facility and community levels. I now work in the Malaria Branch of the U.S. Centers for Disease Control and Prevention in Atlanta. My research interests include infectious diseases, particularly malaria, with a focus on molecular epidemiology and antimalarial resistance.\n\n\nStatistical Background\nI have used R in classes and for some simple statistical analyses and visuals at work, but overall, my experience is very informal, and always involves a lot of googling.) What I love about R is the level of online community engagement and discussion between people using it for such a wide variety of purposes. It can be frustrating at times, but it is by far my favorite statistical software I have worked with.\nI am quite confident in study design, but the data analysis requirements for the research I do at work generally look fairly similar each time, and I have not had the confidence to explore or branch out into more complicated analyses on my own. My aim in participating in this course is to expand my R toolkit and also to gain confidence in using R more in my day to day work. I really love data visualization, so learning more about how to use Tableau with R would be another really exciting opportunity for me.\n\n\nInteresting Fact\nI come from a big, fat, Greek family. I have relatives named Hercules, Antigone, and Socrates to name a few!\n\nData Visualization: When Americans are Happiest\nI have been a long time follower of various data visualization blogs, and one of my favorites is called Flowing Data. They shared a video of an interesting example of creative visualization, using data from the Bureau of Labor Statistics 2021 American Time Use Survey.\nPeople were asked about their happiness throughout the day when they ate, traveled, watched television, took care of kids, and other activities, in addition to reporting happiness on a scale from 0 to 6, where 0 was not happy at all and 6 was very happy. The animation shows the average happiness for the fifty most common activities in individuals from age 20 to 70."
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "DataViz inspiration\nThe data I selected for this exercise in reproducing a visualization comes from the FiveThirtyEight story The Most Common Unisex Names In America: Is Yours One Of Them? The dataset looks the most common unisex names in America through the year 2013, using data from the Social Security Administration.\n\n\n\n\n\nI liked the way this visualization looked because it had a bar graph element which looked embedded into a table. I had to do a little digging online because I was not really familiar with how to do something like this in ggplot2 or any other package, but I did my best to get as close as possible to the final product on the FiveThirtyEight article. Below are the steps I followed to achieve my result!\n\n\nStep 1: Installing and loading packages\n\noptions(repos = list(CRAN=\"http://cran.rstudio.com/\"))\n\n#install\ninstall.packages(\"gt\") \n\n\nThe downloaded binary packages are in\n    /var/folders/gx/td5t8_t911bf6r8cdbtt529r0000gn/T//RtmpVdiT9B/downloaded_packages\n\ninstall.packages(\"gtExtras\")\n\n\nThe downloaded binary packages are in\n    /var/folders/gx/td5t8_t911bf6r8cdbtt529r0000gn/T//RtmpVdiT9B/downloaded_packages\n\n#load\nrequire(readr)\n\nLoading required package: readr\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\nrequire(tidyverse)\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ tidyr   1.3.0     ✔ forcats 1.0.0\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nrequire(gt)\n\nLoading required package: gt\n\nrequire(gtExtras)\n\nLoading required package: gtExtras\n\n\n\n\nStep 2: Data import\n\nunisex_names_table <- read_csv(\"data/unisex_names_table.csv\")\n\nNew names:\nRows: 919 Columns: 6\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): name dbl (5): ...1, total, male_share, female_share, gap\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\nThe data in unisex_names_table.csv contains the over 900 names given to each sex at least one-third of the time and with a minimum of 100 people. It has the following variables:\n\n\n\nHeader\nDefinition\n\n\n\n\nname\nFirst names from the Social Security Administration\n\n\ntotal\nTotal number of living Americans with the name\n\n\nmale_share\nPercentage of people with the name who are male\n\n\nfemale_share\nPercentage of people with the name who are female\n\n\ngap\nGap between male_share and female_share\n\n\n\n\n\nStep 3: Filtering for the data of interest\nSince the visualization only looks at the 20 most popular unisex names, we want to exclude the rest of the names from the dataset that are less popular. Before we do that, let’s do some data wrangling to make things a bit easier and create a duplicate dataset called data. In terms of data wrangling, the main thing I want to do is change the variable name of the first column because currently it is …1, which could cause some errors or confusion. So let’s change it to popularity_rank since it tells us how each name ranks in its overall frequency.\n\nunisex_names_table <- as.data.frame(unisex_names_table)\ndata <- unisex_names_table \ndata <- data.frame(data) \ncolnames(data)[colnames(data) == \"...1\"] = \"popularity_rank\"\n\nSince we are only interested in the top 20 most popular names, we can get rid of all of the less popular names.\n\ntop20 <- data %>% filter(popularity_rank <= 20)\n\nI’m also going to drop the gap variable since it will not be a part of this visualization.\n\ntop20_nogap <- top20 %>%\n    select(popularity_rank, name, total, male_share, female_share)\n\n\n\nStep 4: Ensuring variables are being read by R as the correct type\n\ntop20_nogap <- as.data.frame(top20_nogap)\n  top20_nogap$popularity_rank <- as.numeric(top20_nogap$popularity_rank)\n  top20_nogap$name <- as.factor(top20_nogap$name)\n  top20_nogap$total <- as.numeric(top20_nogap$total)\n  top20_nogap$male_share <- as.numeric(top20_nogap$male_share)\n  top20_nogap$female_share <- as.numeric(top20_nogap$female_share)\n\nThis step is just to ensure that the variables that are numeric are being read as numeric and the variables that are characters are read as characters.\n\n\nStep 5: Creating a gt table\nThe gt extension is used in order to produce high quality tables. The gtExtras extension adds even more functionality so that we can make are table look as much like the example as possible. Let’s call our table tab.\n\ntab <- top20_nogap %>%\n  gt()\n\ntab <- tab %>% \n  #add title and subtitle\n  tab_header(\n    title = \"The most common unisex names in America\",\n    subtitle = md(\"Names for which at least one-third of recipients were male and at least one-third were female, through 2013\")\n  )\n\ntab\n\n\n\n\n\n  \n    \n      The most common unisex names in America\n    \n    \n      Names for which at least one-third of recipients were male and at least one-third were female, through 2013\n    \n  \n  \n    \n      popularity_rank\n      name\n      total\n      male_share\n      female_share\n    \n  \n  \n    1\nCasey\n176544.33\n0.5842866\n0.4157134\n    2\nRiley\n154860.67\n0.5076391\n0.4923609\n    3\nJessie\n136381.83\n0.4778343\n0.5221657\n    4\nJackie\n132928.79\n0.4211326\n0.5788674\n    5\nAvery\n121797.42\n0.3352131\n0.6647869\n    6\nJaime\n109870.19\n0.5617929\n0.4382071\n    7\nPeyton\n94896.40\n0.4337194\n0.5662806\n    8\nKerry\n88963.93\n0.4839488\n0.5160512\n    9\nJody\n80400.52\n0.3520680\n0.6479320\n    10\nKendall\n79210.87\n0.3723667\n0.6276333\n    11\nPayton\n64151.63\n0.3343577\n0.6656423\n    12\nSkyler\n53486.39\n0.6460531\n0.3539469\n    13\nFrankie\n51288.07\n0.6236713\n0.3763287\n    14\nPat\n44781.60\n0.3690344\n0.6309656\n    15\nQuinn\n41920.94\n0.6357419\n0.3642581\n    16\nHarley\n41237.57\n0.5717018\n0.4282982\n    17\nReese\n36360.52\n0.3619103\n0.6380897\n    18\nRobbie\n32636.05\n0.5531569\n0.4468431\n    19\nTommie\n29528.79\n0.6644377\n0.3355623\n    20\nJustice\n27350.56\n0.5281950\n0.4718050\n  \n  \n  \n\n\n\n\n\n\nStep 5: Tweaking our gt table formatting\nNow that we have created a first draft of the table with a title and subtitle that matches our example, let’s make some more changes. First, let’s get the text in an NYT theme so it looks article ready. Second, we want to get rid of all of the decimal places for values under the column titled ‘total.’ This led me to realize I needed to change some column names so that they matched, so that is our last step here.\n\ntab <- tab %>%\n  gtExtras::gt_theme_nytimes() %>% #nyt theme\n  fmt_number(\n    columns = total,\n    decimals = 0) %>% #remove decimal places\n  cols_label(\n    popularity_rank = \" \", #change column names to match example\n    name = \"Name\", \n    total = \"Estimated people with name\", #change name to match \n    male_share = \"Male share\", \n    female_share = \"Female share\")\n\ntab\n\n\n\n\n\n  \n    \n      The most common unisex names in America\n    \n    \n      Names for which at least one-third of recipients were male and at least one-third were female, through 2013\n    \n  \n  \n    \n       \n      Name\n      Estimated people with name\n      Male share\n      Female share\n    \n  \n  \n    1\nCasey\n176,544\n0.5842866\n0.4157134\n    2\nRiley\n154,861\n0.5076391\n0.4923609\n    3\nJessie\n136,382\n0.4778343\n0.5221657\n    4\nJackie\n132,929\n0.4211326\n0.5788674\n    5\nAvery\n121,797\n0.3352131\n0.6647869\n    6\nJaime\n109,870\n0.5617929\n0.4382071\n    7\nPeyton\n94,896\n0.4337194\n0.5662806\n    8\nKerry\n88,964\n0.4839488\n0.5160512\n    9\nJody\n80,401\n0.3520680\n0.6479320\n    10\nKendall\n79,211\n0.3723667\n0.6276333\n    11\nPayton\n64,152\n0.3343577\n0.6656423\n    12\nSkyler\n53,486\n0.6460531\n0.3539469\n    13\nFrankie\n51,288\n0.6236713\n0.3763287\n    14\nPat\n44,782\n0.3690344\n0.6309656\n    15\nQuinn\n41,921\n0.6357419\n0.3642581\n    16\nHarley\n41,238\n0.5717018\n0.4282982\n    17\nReese\n36,361\n0.3619103\n0.6380897\n    18\nRobbie\n32,636\n0.5531569\n0.4468431\n    19\nTommie\n29,529\n0.6644377\n0.3355623\n    20\nJustice\n27,351\n0.5281950\n0.4718050\n  \n  \n  \n\n\n\n\n\n\nStep 6: Incorporating a bar plot into the table\nWe will use the gt_plt_bar_stack function to incorporate bar plots representing the proportion of females and the proportion of males with each of the top 20 names.\n\n#male share\ngt_plt_bar(\ntab,\ncolumn = male_share,\ncolor = \"blue\",\nkeep_column = FALSE,\nwidth = 70,\nscale_type = \"none\",\ntext_color = \"white\"\n)\n\n\n\n\n\n  \n    \n      The most common unisex names in America\n    \n    \n      Names for which at least one-third of recipients were male and at least one-third were female, through 2013\n    \n  \n  \n    \n       \n      Name\n      Estimated people with name\n      Male share\n      Female share\n    \n  \n  \n    1\nCasey\n176,544\n          \n0.4157134\n    2\nRiley\n154,861\n          \n0.4923609\n    3\nJessie\n136,382\n          \n0.5221657\n    4\nJackie\n132,929\n          \n0.5788674\n    5\nAvery\n121,797\n          \n0.6647869\n    6\nJaime\n109,870\n          \n0.4382071\n    7\nPeyton\n94,896\n          \n0.5662806\n    8\nKerry\n88,964\n          \n0.5160512\n    9\nJody\n80,401\n          \n0.6479320\n    10\nKendall\n79,211\n          \n0.6276333\n    11\nPayton\n64,152\n          \n0.6656423\n    12\nSkyler\n53,486\n          \n0.3539469\n    13\nFrankie\n51,288\n          \n0.3763287\n    14\nPat\n44,782\n          \n0.6309656\n    15\nQuinn\n41,921\n          \n0.3642581\n    16\nHarley\n41,238\n          \n0.4282982\n    17\nReese\n36,361\n          \n0.6380897\n    18\nRobbie\n32,636\n          \n0.4468431\n    19\nTommie\n29,529\n          \n0.3355623\n    20\nJustice\n27,351\n          \n0.4718050\n  \n  \n  \n\n\n\n\n\n#female share\ngt_plt_bar(\ntab,\ncolumn = female_share,\ncolor = \"orange\",\nkeep_column = FALSE,\nwidth = 70,\nscale_type = \"none\",\ntext_color = \"white\"\n)\n\n\n\n\n\n  \n    \n      The most common unisex names in America\n    \n    \n      Names for which at least one-third of recipients were male and at least one-third were female, through 2013\n    \n  \n  \n    \n       \n      Name\n      Estimated people with name\n      Male share\n      Female share\n    \n  \n  \n    1\nCasey\n176,544\n0.5842866\n          \n    2\nRiley\n154,861\n0.5076391\n          \n    3\nJessie\n136,382\n0.4778343\n          \n    4\nJackie\n132,929\n0.4211326\n          \n    5\nAvery\n121,797\n0.3352131\n          \n    6\nJaime\n109,870\n0.5617929\n          \n    7\nPeyton\n94,896\n0.4337194\n          \n    8\nKerry\n88,964\n0.4839488\n          \n    9\nJody\n80,401\n0.3520680\n          \n    10\nKendall\n79,211\n0.3723667\n          \n    11\nPayton\n64,152\n0.3343577\n          \n    12\nSkyler\n53,486\n0.6460531\n          \n    13\nFrankie\n51,288\n0.6236713\n          \n    14\nPat\n44,782\n0.3690344\n          \n    15\nQuinn\n41,921\n0.6357419\n          \n    16\nHarley\n41,238\n0.5717018\n          \n    17\nReese\n36,361\n0.3619103\n          \n    18\nRobbie\n32,636\n0.5531569\n          \n    19\nTommie\n29,529\n0.6644377\n          \n    20\nJustice\n27,351\n0.5281950\n          \n  \n  \n  \n\n\n\n\nUnfortunately this does not give us the percentages in one clean table, as in the example visualization. My best resource in this exercise came from an example I found on this website but it didn’t quite get me to the finish line, so I kept looking for other examples. I had a very hard time, so I am still stuck at this point, but if anyone has any suggestions of things to try please let me know!"
  },
  {
    "objectID": "coding_exercise.html",
    "href": "coding_exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "#load packages and check data\nlibrary(\"dslabs\")\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(\"ggplot2\")\n#look at help file for gapminder data\nhelp(gapminder)\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n#Write code that assigns only the African countries to a new object/variable called africadata\n#overview of africadata\n#summary of africadata\n#first new object: infant mortality and life expectancy data for African countries\n#second new object: life expectancy and population data for African countries\n#overview of africa_il and africa_lp\n#summary of africa_il and africa_lp\n#plot life expectancy as a function of infant mortality\n#plot life expectancy as a function of population size\n#identify NA values in infant_mortality\n#since there is NA data prior to 1981 and after 2016, choose a year like 2000 to avoid any NAs #make a dataset only including data from 2000\n#get overview and summary of year_2000\n#new object: infant mortality and life expectancy data for African countries IN 2000\n#new object: life expectancy and population data for African countries IN 2000\n#plot life expectancy as function of infant mortality in 2000 only\n#plot life expectancy as a function of population size with 2000 data\n#fit 2000 data to a linear model (outcome = life_expectancy, predictor = infant_mortality)\n#statistically significant correlation (p=2.83e-8)\n#fit 2000 data to a linear model (outcome = life_expectancy, predictor = population)\n#no statistically significant correlation (p=0.61)"
  },
  {
    "objectID": "coding_exercise.html#this-section-added-by-yao-lu",
    "href": "coding_exercise.html#this-section-added-by-yao-lu",
    "title": "R Coding Exercise",
    "section": "this section added by Yao Lu",
    "text": "this section added by Yao Lu\n\nlibrary('DataExplorer')\nlibrary(ggplot2)\n\ndata clean\n\na <- gapminder\n\nplot_missing(a)\n\n\n\n\nThere 28% missing for gdp, try to narrow the continent to asia to see if that get better\n\nasiadata <- a[which(a$continent=='Asia'),] \nplot_missing(asiadata)\n\n\n\n\nNot better. Here we have 3 choice. Do imputation manually, like substitute missing GDP per capital by the mean of GDP per capital. Use ‘MICE’ package to generate a new data set with automatic imputation. Drop the rows with missing gdp.\nHere for simplicity, I choose drop the missing.\n\nasiadata1 <- asiadata[which(!is.na(asiadata$gdp)),]\nplot_missing(asiadata1)\n\n\n\nasiadata2 <- asiadata1[which(!is.na(asiadata1$infant_mortality)),]\n\nasiadata2$gdppercap <- asiadata2$gdp/asiadata2$population\n\ndata visualization\n\ntable(droplevels(asiadata2$region))\n\n\n      Central Asia       Eastern Asia South-Eastern Asia      Southern Asia \n               125                182                378                353 \n      Western Asia \n               560 \n\nhist(asiadata2$infant_mortality)\n\n\n\nhist(asiadata2$life_expectancy)\n\n\n\nhist(asiadata2$fertility)\n\n\n\nhist(asiadata2$gdppercap)\n\n\n\n\n\np <- ggplot(data = asiadata2, aes(x = year, y = infant_mortality, group= country, color=country))\np + geom_line()+ggtitle(\"infant_mortality\") +  xlab(\"year\") + ylab(\"life time\")\n\n\n\n\nHere we can see all the infant_mortality decrease as the time goes on with different decreasing rate.\n\np <- ggplot(data = asiadata2, aes(x = year, y = life_expectancy, group= country, color=country))\np + geom_line()+ggtitle(\"infant_mortality\") +  xlab(\"year\") + ylab(\"life time\")\n\n\n\n\nHere we can see all the life_expectancy increase as the time goes on with different increasing rate.\ndata modeling\n\nlm1 <-lm(asiadata2$life_expectancy~asiadata2$infant_mortality+asiadata2$fertility+asiadata2$region+asiadata2$gdppercap)\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: asiadata2$life_expectancy\n                             Df Sum Sq Mean Sq   F value    Pr(>F)    \nasiadata2$infant_mortality    1  98874   98874 14217.304 < 2.2e-16 ***\nasiadata2$fertility           1    158     158    22.676 2.092e-06 ***\nasiadata2$region              4   2583     646    92.857 < 2.2e-16 ***\nasiadata2$gdppercap           1    837     837   120.331 < 2.2e-16 ***\nResiduals                  1590  11058       7                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere we can see that all the variables are significant. For detailed relation direction, we will see below.\n\nsummary(lm1)\n\n\nCall:\nlm(formula = asiadata2$life_expectancy ~ asiadata2$infant_mortality + \n    asiadata2$fertility + asiadata2$region + asiadata2$gdppercap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.930  -1.458   0.048   1.419   9.407 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                         7.660e+01  2.721e-01 281.482  < 2e-16 ***\nasiadata2$infant_mortality         -1.605e-01  2.528e-03 -63.485  < 2e-16 ***\nasiadata2$fertility                -5.181e-01  5.613e-02  -9.231  < 2e-16 ***\nasiadata2$regionEastern Asia       -2.196e+00  3.150e-01  -6.971 4.60e-12 ***\nasiadata2$regionSouth-Eastern Asia -1.355e+00  2.771e-01  -4.890 1.11e-06 ***\nasiadata2$regionSouthern Asia       2.651e-01  2.813e-01   0.942    0.346    \nasiadata2$regionWestern Asia        1.292e+00  2.765e-01   4.671 3.24e-06 ***\nasiadata2$gdppercap                 9.079e-05  8.277e-06  10.970  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.637 on 1590 degrees of freedom\nMultiple R-squared:  0.9026,    Adjusted R-squared:  0.9022 \nF-statistic:  2105 on 7 and 1590 DF,  p-value: < 2.2e-16\n\n\nHere we know infant mortality and fertility have negative influence on life time. Gdp per capital have positive influence on life time.\nThe life time in different regions are not all same.\n\n#save the output\noutcome <- broom::tidy(lm1)\n\nfuture work we can do\nDo imputation to make use of the entire data. Find the countries who have the top life time to find what’s their similarities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MADA2023 Data Analysis Portfolio",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio."
  },
  {
    "objectID": "dataanalysis_exercise.html",
    "href": "dataanalysis_exercise.html",
    "title": "Module 4 Data Analysis Exercise",
    "section": "",
    "text": "Summary\nData for this exercise was pulled from the CDC’s data repository. This dataset describes drug poisoning deaths at the county level by selected demographic characteristics and includes age-adjusted death rates for drug poisoning from 1999 to 2015.\nLoading packages, dataset, and reviewing the raw data:\n\n#load packages\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.5.0 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   1.0.1      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(naniar)\n\n#load dataset\ndrug_poison_data <- read_csv(\"dataanalysis-exercise/data/raw/drug_poison_data.csv\")\n\nRows: 53387 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): state, ST, county, death_rate\ndbl (4): FIPS, year, FIPS_state, population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndrug_poison_data <- read_csv(\"dataanalysis-exercise/data/raw_data/drug_poison_data.csv\")\n\nRows: 53387 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): state, ST, county, death_rate\ndbl (4): FIPS, year, FIPS_state, population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#examine data\nsummary(drug_poison_data)\n\n      FIPS            year         state                ST           \n Min.   : 1001   Min.   :1999   Length:53387       Length:53387      \n 1st Qu.:18181   1st Qu.:2003   Class :character   Class :character  \n Median :29179   Median :2007   Mode  :character   Mode  :character  \n Mean   :30411   Mean   :2007                                        \n 3rd Qu.:45083   3rd Qu.:2011                                        \n Max.   :56045   Max.   :2015                                        \n                                                                     \n   FIPS_state       county            population        death_rate       \n Min.   : 1.00   Length:53387       Min.   :      55   Length:53387      \n 1st Qu.:18.00   Class :character   1st Qu.:   11114   Class :character  \n Median :29.00   Mode  :character   Median :   25455   Mode  :character  \n Mean   :30.31                      Mean   :   95806                     \n 3rd Qu.:45.00                      3rd Qu.:   65101                     \n Max.   :56.00                      Max.   :10170292                     \n                                    NA's   :4                            \n\n\nCreating a new dataframe called “data” with variables of interest:\nThe selected variables here are year, state, county, population and death rate.\n\n#refined dataset with only variables of interest\ndata <- drug_poison_data %>%\n    select(year, state, county, population, death_rate)\n\nFilter to look at only cases in Georgia:\n\ngeorgia <- data %>%\n  filter(state %in% \"Georgia\")\n\nThis new final dataset (georgia) gives us cases of drug poisoning from 1999-2015 in the state of Georgia.\nSave cleaned data as an RDS file in processed_data folder:\n\nsaveRDS(georgia, file=\"dataanalysis-exercise/data/processed/georgia_clean.rds\")\nsaveRDS(georgia, file=\"dataanalysis-exercise/data/processed_data/georgia_clean.rds\")\n\nSave summary table:\n\n#make summary table\nsummary_georgia = data.frame(do.call(cbind, lapply(georgia, summary)))\nprint(summary_georgia)\n\n        year     state    county       population death_rate\nMin.    1999      2703      2703             1639       2703\n1st Qu. 2003 character character          11227.5  character\nMedian  2007 character character            22092  character\nMean    2007      2703      2703 58029.9470958195       2703\n3rd Qu. 2011 character character            47888  character\nMax.    2015 character character          1010562  character\n\n#save it as an RDS file\nsaveRDS(summary_georgia, file=\"dataanalysis-exercise/results/summary_georgia.rds\")\n\n—————————————\nTHIS SECTION ADDED BY NICOLE LUISI\n—————————————\n\nLoad required packages\n\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(dplyr)\n\n\n\nRead in clean RDS file and review\n\ngeorgia_clean_NL <- readRDS(here(\"dataanalysis-exercise\", \"data\", \"processed_data\", \"georgia_clean.rds\"))\n\nstr(georgia_clean_NL)\n\ntibble [2,703 × 5] (S3: tbl_df/tbl/data.frame)\n $ year      : num [1:2703] 2014 2011 2010 2015 2010 ...\n $ state     : chr [1:2703] \"Georgia\" \"Georgia\" \"Georgia\" \"Georgia\" ...\n $ county    : chr [1:2703] \"Union County, GA\" \"Walker County, GA\" \"Fulton County, GA\" \"Greene County, GA\" ...\n $ population: num [1:2703] 21952 68617 926038 16710 9093 ...\n $ death_rate: chr [1:2703] \"18.1-20\" \"14.1-16\" \"10.1-12\" \"10.1-12\" ...\n\nhead(georgia_clean_NL)\n\n# A tibble: 6 × 5\n   year state   county                population death_rate\n  <dbl> <chr>   <chr>                      <dbl> <chr>     \n1  2014 Georgia Union County, GA           21952 18.1-20   \n2  2011 Georgia Walker County, GA          68617 14.1-16   \n3  2010 Georgia Fulton County, GA         926038 10.1-12   \n4  2015 Georgia Greene County, GA          16710 10.1-12   \n5  2010 Georgia Montgomery County, GA       9093 8.1-10    \n6  2012 Georgia Worth County, GA           21464 8.1-10    \n\n\n\n\nCan’t work with death_rate variable as character range so splitting into numeric min and max variables\n\ngeorgia_clean_NL$death_rate_min <- as.numeric(str_extract(georgia_clean_NL$death_rate, \"[^-]+\"))\n\nWarning: NAs introduced by coercion\n\ngeorgia_clean_NL$death_rate_max <- as.numeric(str_extract(georgia_clean_NL$death_rate, '\\\\b\\\\w+$'))\n\n# Check min and max\nhead(georgia_clean_NL[,c(\"death_rate\", \"death_rate_min\", \"death_rate_max\")])\n\n# A tibble: 6 × 3\n  death_rate death_rate_min death_rate_max\n  <chr>               <dbl>          <dbl>\n1 18.1-20              18.1             20\n2 14.1-16              14.1             16\n3 10.1-12              10.1             12\n4 10.1-12              10.1             12\n5 8.1-10                8.1             10\n6 8.1-10                8.1             10\n\n\n\n\nCreate subsets for Dekalb and Fulton county\n\ngeorgia_clean_NL_dekalb <- georgia_clean_NL %>%\n  filter(county == \"DeKalb County, GA\")\ngeorgia_clean_NL_dekalb <- georgia_clean_NL_dekalb[,c(\"year\", \"county\",\"death_rate_min\", \"death_rate_max\")]\ngeorgia_clean_NL_fulton <- georgia_clean_NL %>%\n  filter(county == \"Fulton County, GA\")\ngeorgia_clean_NL_fulton <- georgia_clean_NL_fulton[,c(\"year\", \"county\",\"death_rate_min\", \"death_rate_max\")]\n\n\n\nPlot min and max death rates for Dekalb and Fulton county\n\n# Transform to long format\ngeorgia_clean_NL_dekalb_long <- gather(georgia_clean_NL_dekalb, group, value, death_rate_min:death_rate_max) %>% arrange(factor(year, levels = c(\"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"))) %>% \n  mutate(year=factor(year, levels=unique(year)))\n\ngeorgia_clean_NL_fulton_long <- gather(georgia_clean_NL_fulton, group, value, death_rate_min:death_rate_max) %>% arrange(factor(year, levels = c(\"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"))) %>% \n  mutate(year=factor(year, levels=unique(year)))\n\n# Dekalb plot\ndekalb_plot <- ggplot(georgia_clean_NL_dekalb_long, aes(x = year, y = value, fill = group)) +\n  geom_col(position = \"dodge\", colour = \"black\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  ylab(\"Year\") +\n  xlab(\"Age-Adjusted Death Rate for Drug Poisoning\") +  \n  ggtitle(\"Dekalb County\") + \n  scale_y_continuous(breaks=seq(0, 15, 5))\n\n# Fulton plot\nfulton_plot <- ggplot(georgia_clean_NL_fulton_long, aes(x = year, y = value, fill = group)) +\n  geom_col(position = \"dodge\", colour = \"black\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  ylab(\"Year\") +\n  xlab(\"Age-Adjusted Death Rate for Drug Poisoning\") +  \n  ggtitle(\"Fulton County\") + \n  scale_y_continuous(breaks=seq(0, 15, 15))\n\n# Grid of plots\ngrid.arrange(dekalb_plot, fulton_plot, ncol = 1)\n\n\n\n\n\n\nCreate objects with overall min and max by year for 2015 and 2000\n\nyear_minmax <- georgia_clean_NL %>% select(year, death_rate_min, death_rate_max)\n\nyear_minmax1 <- year_minmax %>%\n  group_by(year) %>%\n  summarise(\n    MaxByYear = max(death_rate_max, na.rm = T),\n    MinByYear = min(death_rate_min, na.rm = T)\n  ) %>%\n  arrange(year)\n\n\n\nPlot overall (lowest) min and (highest) max death rate by year\n\nplot(year_minmax1$year,                             \n     year_minmax1$MinByYear,\n     main = \"Overall Min and Max Death Rate by Year, All Counties\",\n     type = \"l\",\n     col = 2,\n     ylim = c(- 15, 40),\n     xlab = \"Year\",\n     ylab = \"Death Rate\")\nlines(year_minmax1$year,                            \n      year_minmax1$MaxByYear,\n      type = \"l\",\n      col = 3)\nlegend(\"topright\",                          \n       c(\"Min Death Rate\", \"Max Death Rate\"),\n       lty = 1,\n       col = 2:4)"
  }
]