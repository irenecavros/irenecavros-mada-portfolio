[
  {
    "objectID": "dataanalysis-exercise/results/readme.html",
    "href": "dataanalysis-exercise/results/readme.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "dataanalysis-exercise/data/readme.html",
    "href": "dataanalysis-exercise/data/readme.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Introduction\nThis week’s data was all about the age difference in years between Hollywood movie love interests. More information on the data can be found at the Hollywood Age Gap website via Data Is Plural.\n\n\nLoad packages\n\nlibrary('tidyverse')\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary('ggplot2')\nlibrary(\"dplyr\")\nlibrary('lubridate')\n\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(\"cowplot\")\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(\"plotly\")\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(\"forcats\")\n\n\n\nGet the data\n\nlibrary(tidytuesdayR) #read in `tidytuesdayR`\ntuesdata <- tidytuesdayR::tt_load('2023-02-14') #load in readme and datasets for this week\n\n--- Compiling #TidyTuesday Information for 2023-02-14 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `age_gaps.csv`\n\n\n--- Download complete ---\n\ntuesdata <- tidytuesdayR::tt_load(2023, week = 7) \n\n--- Compiling #TidyTuesday Information for 2023-02-14 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `age_gaps.csv`\n\n\n--- Download complete ---\n\nage_gaps <- tuesdata$age_gaps\n\n\n\nInitial data exploration\nSummary statistics for each variable\n\nsummary(age_gaps)\n\n  movie_name         release_year    director         age_difference \n Length:1155        Min.   :1935   Length:1155        Min.   : 0.00  \n Class :character   1st Qu.:1997   Class :character   1st Qu.: 4.00  \n Mode  :character   Median :2004   Mode  :character   Median : 8.00  \n                    Mean   :2001                      Mean   :10.42  \n                    3rd Qu.:2012                      3rd Qu.:15.00  \n                    Max.   :2022                      Max.   :52.00  \n couple_number   actor_1_name       actor_2_name       character_1_gender\n Min.   :1.000   Length:1155        Length:1155        Length:1155       \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.398                                                           \n 3rd Qu.:2.000                                                           \n Max.   :7.000                                                           \n character_2_gender actor_1_birthdate    actor_2_birthdate     actor_1_age   \n Length:1155        Min.   :1889-04-16   Min.   :1906-10-06   Min.   :18.00  \n Class :character   1st Qu.:1953-05-16   1st Qu.:1965-03-25   1st Qu.:33.00  \n Mode  :character   Median :1964-10-03   Median :1974-07-30   Median :39.00  \n                    Mean   :1960-09-07   Mean   :1971-01-29   Mean   :40.64  \n                    3rd Qu.:1973-08-07   3rd Qu.:1982-04-07   3rd Qu.:47.00  \n                    Max.   :1996-06-01   Max.   :1996-11-11   Max.   :81.00  \n  actor_2_age   \n Min.   :17.00  \n 1st Qu.:25.00  \n Median :29.00  \n Mean   :30.21  \n 3rd Qu.:34.00  \n Max.   :68.00  \n\n\nLook at variable types\n\nstr(age_gaps)\n\nspc_tbl_ [1,155 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ movie_name        : chr [1:1155] \"Harold and Maude\" \"Venus\" \"The Quiet American\" \"The Big Lebowski\" ...\n $ release_year      : num [1:1155] 1971 2006 2002 1998 2010 ...\n $ director          : chr [1:1155] \"Hal Ashby\" \"Roger Michell\" \"Phillip Noyce\" \"Joel Coen\" ...\n $ age_difference    : num [1:1155] 52 50 49 45 43 42 40 39 38 38 ...\n $ couple_number     : num [1:1155] 1 1 1 1 1 1 1 1 1 1 ...\n $ actor_1_name      : chr [1:1155] \"Ruth Gordon\" \"Peter O'Toole\" \"Michael Caine\" \"David Huddleston\" ...\n $ actor_2_name      : chr [1:1155] \"Bud Cort\" \"Jodie Whittaker\" \"Do Thi Hai Yen\" \"Tara Reid\" ...\n $ character_1_gender: chr [1:1155] \"woman\" \"man\" \"man\" \"man\" ...\n $ character_2_gender: chr [1:1155] \"man\" \"woman\" \"woman\" \"woman\" ...\n $ actor_1_birthdate : Date[1:1155], format: \"1896-10-30\" \"1932-08-02\" ...\n $ actor_2_birthdate : Date[1:1155], format: \"1948-03-29\" \"1982-06-03\" ...\n $ actor_1_age       : num [1:1155] 75 74 69 68 81 59 62 69 57 77 ...\n $ actor_2_age       : num [1:1155] 23 24 20 23 38 17 22 30 19 39 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   movie_name = col_character(),\n  ..   release_year = col_double(),\n  ..   director = col_character(),\n  ..   age_difference = col_double(),\n  ..   couple_number = col_double(),\n  ..   actor_1_name = col_character(),\n  ..   actor_2_name = col_character(),\n  ..   character_1_gender = col_character(),\n  ..   character_2_gender = col_character(),\n  ..   actor_1_birthdate = col_date(format = \"\"),\n  ..   actor_2_birthdate = col_date(format = \"\"),\n  ..   actor_1_age = col_double(),\n  ..   actor_2_age = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nTable of age_gaps data\n\nhead(age_gaps)\n\n# A tibble: 6 × 13\n  movie_name     relea…¹ direc…² age_d…³ coupl…⁴ actor…⁵ actor…⁶ chara…⁷ chara…⁸\n  <chr>            <dbl> <chr>     <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>  \n1 Harold and Ma…    1971 Hal As…      52       1 Ruth G… Bud Co… woman   man    \n2 Venus             2006 Roger …      50       1 Peter … Jodie … man     woman  \n3 The Quiet Ame…    2002 Philli…      49       1 Michae… Do Thi… man     woman  \n4 The Big Lebow…    1998 Joel C…      45       1 David … Tara R… man     woman  \n5 Beginners         2010 Mike M…      43       1 Christ… Goran … man     man    \n6 Poison Ivy        1992 Katt S…      42       1 Tom Sk… Drew B… man     woman  \n# … with 4 more variables: actor_1_birthdate <date>, actor_2_birthdate <date>,\n#   actor_1_age <dbl>, actor_2_age <dbl>, and abbreviated variable names\n#   ¹​release_year, ²​director, ³​age_difference, ⁴​couple_number, ⁵​actor_1_name,\n#   ⁶​actor_2_name, ⁷​character_1_gender, ⁸​character_2_gender\n\n\n\n\nAnalysis ideas\nBased on my initial exploration, I came up with a few ideas for potentially interesting research questions:\n\nAre there trends in time when we look at age gap? For example, has the gap widened or narrowed over time?\nHow do the data look if we were to look at age gaps by director?\nHow do the age gaps look if we were to compare films where actor 1 (the older actor) was female versus films where actor 1 was male\n\n\n\nDuplicating dataset\nBefore I do any manipulating, I will duplicate the original age_gaps dataset into a new dataset titled data.\n\ndata <- age_gaps\n\n\n\nTrends in time\nLet’s try and do a historical analysis of age differences based on the year each film was released.\n\nplot(data$release_year, data$age_difference)\n\n\n\n\nThis is a bit difficult to discern any information or conclusions from, other than it appears that the number of movies produced each year has increased!\nLet’s see if we can take an average of the age gaps for each year and create a better plot.\nThis first step will create a new data frame with each year and the average age gap for that year. Not all years are represented in the dataset of movies, so there are some years which do not appear in the table.\n\ngap_byyear <- data %>%\n  group_by(release_year) %>%\n  summarise_at(vars(age_difference), list(year_meandiff = mean))\ngap_byyear\n\n# A tibble: 82 × 2\n   release_year year_meandiff\n          <dbl>         <dbl>\n 1         1935         13   \n 2         1936         21   \n 3         1937          7.33\n 4         1939         12   \n 5         1940         11.3 \n 6         1942         20.5 \n 7         1944         25   \n 8         1946         25   \n 9         1947         25   \n10         1948         23.2 \n# … with 72 more rows\n\n\nNow let’s try plotting again using the average age difference for each year rather than a having a data point for each film in that year.\n\nggplot(gap_byyear, aes(x = release_year, y = year_meandiff)) +\n  geom_line () + \n  theme_classic()\n\n\n\n\nHere we can see the data as box plot visualization:\n\nboxplot(age_difference ~ release_year, data = data)\n\n\n\n\n\n\nage_gaps by director\nLet’s do something similar to what we did above by calculating average age gaps for each year, but this time we can do it by director.\n\ngap_bydirector <- data %>%\n  group_by(director) %>%\n  summarise_at(vars(age_difference), list(director_meandiff = mean))\ngap_bydirector\n\n# A tibble: 510 × 2\n   director                    director_meandiff\n   <chr>                                   <dbl>\n 1 Abdellatif Kechiche                      8   \n 2 Adam McKay                               6.67\n 3 Adam Nee, Aaron Nee                     16   \n 4 Adam Shankman                           12.2 \n 5 Adrian Lyne                             12.8 \n 6 Alan Parker                             15   \n 7 Alan Shapiro                             7   \n 8 Alejandro Agresti                        0   \n 9 Alejandro González Iñárritu             19   \n10 Alex Garland                             4.67\n# … with 500 more rows\n\n\nFrom here let’s just take the top 10 directors with the highest age gap and create a bar plot of their average age gap.\n\ntop10_directors <-  gap_bydirector %>% \n  top_n(10)\n\nSelecting by director_meandiff\n\nggplot(top10_directors, aes(x = director_meandiff, y = director)) +\n  geom_col () + \n  theme_classic()"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Bio\n\n\n\n\n\nI am a second year part-time PhD student in Epidemiology at UGA. I have spent over five years working in sub-Saharan Africa on both HIV and malaria programs at the health facility and community levels. I now work in the Malaria Branch of the U.S. Centers for Disease Control and Prevention in Atlanta. My research interests include infectious diseases, particularly malaria, with a focus on molecular epidemiology and antimalarial resistance.\n\n\nStatistical Background\nI have used R in classes and for some simple statistical analyses and visuals at work, but overall, my experience is very informal, and always involves a lot of googling.) What I love about R is the level of online community engagement and discussion between people using it for such a wide variety of purposes. It can be frustrating at times, but it is by far my favorite statistical software I have worked with.\nI am quite confident in study design, but the data analysis requirements for the research I do at work generally look fairly similar each time, and I have not had the confidence to explore or branch out into more complicated analyses on my own. My aim in participating in this course is to expand my R toolkit and also to gain confidence in using R more in my day to day work. I really love data visualization, so learning more about how to use Tableau with R would be another really exciting opportunity for me.\n\n\nInteresting Fact\nI come from a big, fat, Greek family. I have relatives named Hercules, Antigone, and Socrates to name a few!\n\nData Visualization: When Americans are Happiest\nI have been a long time follower of various data visualization blogs, and one of my favorites is called Flowing Data. They shared a video of an interesting example of creative visualization, using data from the Bureau of Labor Statistics 2021 American Time Use Survey.\nPeople were asked about their happiness throughout the day when they ate, traveled, watched television, took care of kids, and other activities, in addition to reporting happiness on a scale from 0 to 6, where 0 was not happy at all and 6 was very happy. The animation shows the average happiness for the fifty most common activities in individuals from age 20 to 70."
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "DataViz inspiration\nThe data I selected for this exercise in reproducing a visualization comes from the FiveThirtyEight story The Most Common Unisex Names In America: Is Yours One Of Them? The dataset looks the most common unisex names in America through the year 2013, using data from the Social Security Administration.\n\n\n\n\n\nI liked the way this visualization looked because it had a bar graph element which looked embedded into a table. I had to do a little digging online because I was not really familiar with how to do something like this in ggplot2 or any other package, but I did my best to get as close as possible to the final product on the FiveThirtyEight article. Below are the steps I followed to achieve my result!\n\n\nStep 1: Installing and loading packages\n\noptions(repos = list(CRAN=\"http://cran.rstudio.com/\"))\n\n#install\ninstall.packages(\"gt\") \n\n\nThe downloaded binary packages are in\n    /var/folders/gx/td5t8_t911bf6r8cdbtt529r0000gn/T//RtmpOgfsHg/downloaded_packages\n\ninstall.packages(\"gtExtras\")\n\n\nThe downloaded binary packages are in\n    /var/folders/gx/td5t8_t911bf6r8cdbtt529r0000gn/T//RtmpOgfsHg/downloaded_packages\n\ninstall.packages(\"svglite\")\n\n\nThe downloaded binary packages are in\n    /var/folders/gx/td5t8_t911bf6r8cdbtt529r0000gn/T//RtmpOgfsHg/downloaded_packages\n\n#load\nrequire(readr)\n\nLoading required package: readr\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\nrequire(tidyverse)\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ tidyr   1.3.0     ✔ forcats 1.0.0\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nrequire(gt)\n\nLoading required package: gt\n\nrequire(gtExtras)\n\nLoading required package: gtExtras\n\nrequire(svglite)\n\nLoading required package: svglite\n\n\n\n\nStep 2: Data import\n\nunisex_names_table <- read_csv(\"data/unisex_names_table.csv\")\n\nNew names:\nRows: 919 Columns: 6\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): name dbl (5): ...1, total, male_share, female_share, gap\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\nThe data in unisex_names_table.csv contains the over 900 names given to each sex at least one-third of the time and with a minimum of 100 people. It has the following variables:\n\n\n\nHeader\nDefinition\n\n\n\n\nname\nFirst names from the Social Security Administration\n\n\ntotal\nTotal number of living Americans with the name\n\n\nmale_share\nPercentage of people with the name who are male\n\n\nfemale_share\nPercentage of people with the name who are female\n\n\ngap\nGap between male_share and female_share\n\n\n\n\n\nStep 3: Filtering for the data of interest\nSince the visualization only looks at the 20 most popular unisex names, we want to exclude the rest of the names from the dataset that are less popular. Before we do that, let’s do some data wrangling to make things a bit easier and create a duplicate dataset called data. In terms of data wrangling, the main thing I want to do is change the variable name of the first column because currently it is …1, which could cause some errors or confusion. So let’s change it to popularity_rank since it tells us how each name ranks in its overall frequency.\n\nunisex_names_table <- as.data.frame(unisex_names_table)\ndata <- unisex_names_table \ndata <- data.frame(data) \ncolnames(data)[colnames(data) == \"...1\"] = \"popularity_rank\"\n\nSince we are only interested in the top 20 most popular names, we can get rid of all of the less popular names.\n\ntop20 <- data %>% filter(popularity_rank <= 20)\n\nI’m also going to drop the gap variable since it will not be a part of this visualization.\n\ntop20_nogap <- top20 %>%\n    select(popularity_rank, name, total, male_share, female_share)\n\n\n\nStep 4: Ensuring variables are being read by R as the correct type\n\ntop20_nogap <- as.data.frame(top20_nogap)\n  top20_nogap$popularity_rank <- as.numeric(top20_nogap$popularity_rank)\n  top20_nogap$name <- as.factor(top20_nogap$name)\n  top20_nogap$total <- as.numeric(top20_nogap$total)\n  top20_nogap$male_share <- as.numeric(top20_nogap$male_share)\n  top20_nogap$female_share <- as.numeric(top20_nogap$female_share)\n\nThis step is just to ensure that the variables that are numeric are being read as numeric and the variables that are characters are read as characters.\n\n\nStep 5: Creating a gt table\nThe gt extension is used in order to produce high quality tables. The gtExtras extension adds even more functionality so that we can make are table look as much like the example as possible. Let’s call our table tab.\n\ntab <- top20_nogap %>%\n  gt()\n\ntab <- tab %>% \n  #add title and subtitle\n  tab_header(\n    title = \"The most common unisex names in America\",\n    subtitle = md(\"Names for which at least one-third of recipients were male and at least one-third were female, through 2013\")\n  )\n\ntab\n\n\n\n\n\n  \n    \n      The most common unisex names in America\n    \n    \n      Names for which at least one-third of recipients were male and at least one-third were female, through 2013\n    \n  \n  \n    \n      popularity_rank\n      name\n      total\n      male_share\n      female_share\n    \n  \n  \n    1\nCasey\n176544.33\n0.5842866\n0.4157134\n    2\nRiley\n154860.67\n0.5076391\n0.4923609\n    3\nJessie\n136381.83\n0.4778343\n0.5221657\n    4\nJackie\n132928.79\n0.4211326\n0.5788674\n    5\nAvery\n121797.42\n0.3352131\n0.6647869\n    6\nJaime\n109870.19\n0.5617929\n0.4382071\n    7\nPeyton\n94896.40\n0.4337194\n0.5662806\n    8\nKerry\n88963.93\n0.4839488\n0.5160512\n    9\nJody\n80400.52\n0.3520680\n0.6479320\n    10\nKendall\n79210.87\n0.3723667\n0.6276333\n    11\nPayton\n64151.63\n0.3343577\n0.6656423\n    12\nSkyler\n53486.39\n0.6460531\n0.3539469\n    13\nFrankie\n51288.07\n0.6236713\n0.3763287\n    14\nPat\n44781.60\n0.3690344\n0.6309656\n    15\nQuinn\n41920.94\n0.6357419\n0.3642581\n    16\nHarley\n41237.57\n0.5717018\n0.4282982\n    17\nReese\n36360.52\n0.3619103\n0.6380897\n    18\nRobbie\n32636.05\n0.5531569\n0.4468431\n    19\nTommie\n29528.79\n0.6644377\n0.3355623\n    20\nJustice\n27350.56\n0.5281950\n0.4718050\n  \n  \n  \n\n\n\n\n\n\nStep 5: Tweaking our gt table formatting\nNow that we have created a first draft of the table with a title and subtitle that matches our example, let’s make some more changes. First, let’s get the text in an NYT theme so it looks article ready. Second, we want to get rid of all of the decimal places for values under the column titled ‘total.’ This led me to realize I needed to change some column names so that they matched, so that is our last step here.\n\ntab <- tab %>%\n  gtExtras::gt_theme_nytimes() %>% #nyt theme\n  fmt_number(\n    columns = total,\n    decimals = 0) %>% #remove decimal places\n  cols_label(\n    popularity_rank = \" \", #change column names to match example\n    name = \"Name\", \n    total = \"Estimated people with name\", #change name to match \n    male_share = \"Male share\", \n    female_share = \"Female share\")\n\ntab\n\n\n\n\n\n  \n    \n      The most common unisex names in America\n    \n    \n      Names for which at least one-third of recipients were male and at least one-third were female, through 2013\n    \n  \n  \n    \n       \n      Name\n      Estimated people with name\n      Male share\n      Female share\n    \n  \n  \n    1\nCasey\n176,544\n0.5842866\n0.4157134\n    2\nRiley\n154,861\n0.5076391\n0.4923609\n    3\nJessie\n136,382\n0.4778343\n0.5221657\n    4\nJackie\n132,929\n0.4211326\n0.5788674\n    5\nAvery\n121,797\n0.3352131\n0.6647869\n    6\nJaime\n109,870\n0.5617929\n0.4382071\n    7\nPeyton\n94,896\n0.4337194\n0.5662806\n    8\nKerry\n88,964\n0.4839488\n0.5160512\n    9\nJody\n80,401\n0.3520680\n0.6479320\n    10\nKendall\n79,211\n0.3723667\n0.6276333\n    11\nPayton\n64,152\n0.3343577\n0.6656423\n    12\nSkyler\n53,486\n0.6460531\n0.3539469\n    13\nFrankie\n51,288\n0.6236713\n0.3763287\n    14\nPat\n44,782\n0.3690344\n0.6309656\n    15\nQuinn\n41,921\n0.6357419\n0.3642581\n    16\nHarley\n41,238\n0.5717018\n0.4282982\n    17\nReese\n36,361\n0.3619103\n0.6380897\n    18\nRobbie\n32,636\n0.5531569\n0.4468431\n    19\nTommie\n29,529\n0.6644377\n0.3355623\n    20\nJustice\n27,351\n0.5281950\n0.4718050\n  \n  \n  \n\n\n\n\n\n\nStep 6: Incorporating a bar plot into the table\nWe will use the gt_plt_bar function to incorporate bar plots representing the proportion of females and the proportion of males with each of the top 20 names.\n\n#male share\ngt_plt_bar(\ntab,\ncolumn = male_share,\ncolor = \"blue\",\nkeep_column = FALSE,\nwidth = 70,\nscale_type = \"none\",\ntext_color = \"white\"\n)\n\n\n\n\n\n  \n    \n      The most common unisex names in America\n    \n    \n      Names for which at least one-third of recipients were male and at least one-third were female, through 2013\n    \n  \n  \n    \n       \n      Name\n      Estimated people with name\n      Male share\n      Female share\n    \n  \n  \n    1\nCasey\n176,544\n          \n0.4157134\n    2\nRiley\n154,861\n          \n0.4923609\n    3\nJessie\n136,382\n          \n0.5221657\n    4\nJackie\n132,929\n          \n0.5788674\n    5\nAvery\n121,797\n          \n0.6647869\n    6\nJaime\n109,870\n          \n0.4382071\n    7\nPeyton\n94,896\n          \n0.5662806\n    8\nKerry\n88,964\n          \n0.5160512\n    9\nJody\n80,401\n          \n0.6479320\n    10\nKendall\n79,211\n          \n0.6276333\n    11\nPayton\n64,152\n          \n0.6656423\n    12\nSkyler\n53,486\n          \n0.3539469\n    13\nFrankie\n51,288\n          \n0.3763287\n    14\nPat\n44,782\n          \n0.6309656\n    15\nQuinn\n41,921\n          \n0.3642581\n    16\nHarley\n41,238\n          \n0.4282982\n    17\nReese\n36,361\n          \n0.6380897\n    18\nRobbie\n32,636\n          \n0.4468431\n    19\nTommie\n29,529\n          \n0.3355623\n    20\nJustice\n27,351\n          \n0.4718050\n  \n  \n  \n\n\n\n\n\n#female share\ngt_plt_bar(\ntab,\ncolumn = female_share,\ncolor = \"orange\",\nkeep_column = FALSE,\nwidth = 70,\nscale_type = \"none\",\ntext_color = \"white\"\n)\n\n\n\n\n\n  \n    \n      The most common unisex names in America\n    \n    \n      Names for which at least one-third of recipients were male and at least one-third were female, through 2013\n    \n  \n  \n    \n       \n      Name\n      Estimated people with name\n      Male share\n      Female share\n    \n  \n  \n    1\nCasey\n176,544\n0.5842866\n          \n    2\nRiley\n154,861\n0.5076391\n          \n    3\nJessie\n136,382\n0.4778343\n          \n    4\nJackie\n132,929\n0.4211326\n          \n    5\nAvery\n121,797\n0.3352131\n          \n    6\nJaime\n109,870\n0.5617929\n          \n    7\nPeyton\n94,896\n0.4337194\n          \n    8\nKerry\n88,964\n0.4839488\n          \n    9\nJody\n80,401\n0.3520680\n          \n    10\nKendall\n79,211\n0.3723667\n          \n    11\nPayton\n64,152\n0.3343577\n          \n    12\nSkyler\n53,486\n0.6460531\n          \n    13\nFrankie\n51,288\n0.6236713\n          \n    14\nPat\n44,782\n0.3690344\n          \n    15\nQuinn\n41,921\n0.6357419\n          \n    16\nHarley\n41,238\n0.5717018\n          \n    17\nReese\n36,361\n0.3619103\n          \n    18\nRobbie\n32,636\n0.5531569\n          \n    19\nTommie\n29,529\n0.6644377\n          \n    20\nJustice\n27,351\n0.5281950\n          \n  \n  \n  \n\n\n\n\nUnfortunately this does not give us the percentages in one clean table, as in the example visualization. I played around with gt_plt_bar_stack to try and get one stacked plot vs. two separate ones but did not have any luck so I ended up only using gt_plt_bar. My best resource in this exercise came from an example I found on this website but it didn’t quite get me to the finish line, so I kept looking for other examples. I had a very hard time, so I am still stuck at this point, but if anyone has any suggestions of things to try please let me know!"
  },
  {
    "objectID": "coding_exercise.html",
    "href": "coding_exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "#load packages and check data\nlibrary(\"dslabs\")\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(\"ggplot2\")\n#look at help file for gapminder data\nhelp(gapminder)\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n#Write code that assigns only the African countries to a new object/variable called africadata\n#overview of africadata\n#summary of africadata\n#first new object: infant mortality and life expectancy data for African countries\n#second new object: life expectancy and population data for African countries\n#overview of africa_il and africa_lp\n#summary of africa_il and africa_lp\n#plot life expectancy as a function of infant mortality\n#plot life expectancy as a function of population size\n#identify NA values in infant_mortality\n#since there is NA data prior to 1981 and after 2016, choose a year like 2000 to avoid any NAs #make a dataset only including data from 2000\n#get overview and summary of year_2000\n#new object: infant mortality and life expectancy data for African countries IN 2000\n#new object: life expectancy and population data for African countries IN 2000\n#plot life expectancy as function of infant mortality in 2000 only\n#plot life expectancy as a function of population size with 2000 data\n#fit 2000 data to a linear model (outcome = life_expectancy, predictor = infant_mortality)\n#statistically significant correlation (p=2.83e-8)\n#fit 2000 data to a linear model (outcome = life_expectancy, predictor = population)\n#no statistically significant correlation (p=0.61)"
  },
  {
    "objectID": "coding_exercise.html#this-section-added-by-yao-lu",
    "href": "coding_exercise.html#this-section-added-by-yao-lu",
    "title": "R Coding Exercise",
    "section": "this section added by Yao Lu",
    "text": "this section added by Yao Lu\n\nlibrary('DataExplorer')\nlibrary(ggplot2)\n\ndata clean\n\na <- gapminder\n\nplot_missing(a)\n\n\n\n\nThere 28% missing for gdp, try to narrow the continent to asia to see if that get better\n\nasiadata <- a[which(a$continent=='Asia'),] \nplot_missing(asiadata)\n\n\n\n\nNot better. Here we have 3 choice. Do imputation manually, like substitute missing GDP per capital by the mean of GDP per capital. Use ‘MICE’ package to generate a new data set with automatic imputation. Drop the rows with missing gdp.\nHere for simplicity, I choose drop the missing.\n\nasiadata1 <- asiadata[which(!is.na(asiadata$gdp)),]\nplot_missing(asiadata1)\n\n\n\nasiadata2 <- asiadata1[which(!is.na(asiadata1$infant_mortality)),]\n\nasiadata2$gdppercap <- asiadata2$gdp/asiadata2$population\n\ndata visualization\n\ntable(droplevels(asiadata2$region))\n\n\n      Central Asia       Eastern Asia South-Eastern Asia      Southern Asia \n               125                182                378                353 \n      Western Asia \n               560 \n\nhist(asiadata2$infant_mortality)\n\n\n\nhist(asiadata2$life_expectancy)\n\n\n\nhist(asiadata2$fertility)\n\n\n\nhist(asiadata2$gdppercap)\n\n\n\n\n\np <- ggplot(data = asiadata2, aes(x = year, y = infant_mortality, group= country, color=country))\np + geom_line()+ggtitle(\"infant_mortality\") +  xlab(\"year\") + ylab(\"life time\")\n\n\n\n\nHere we can see all the infant_mortality decrease as the time goes on with different decreasing rate.\n\np <- ggplot(data = asiadata2, aes(x = year, y = life_expectancy, group= country, color=country))\np + geom_line()+ggtitle(\"infant_mortality\") +  xlab(\"year\") + ylab(\"life time\")\n\n\n\n\nHere we can see all the life_expectancy increase as the time goes on with different increasing rate.\ndata modeling\n\nlm1 <-lm(asiadata2$life_expectancy~asiadata2$infant_mortality+asiadata2$fertility+asiadata2$region+asiadata2$gdppercap)\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: asiadata2$life_expectancy\n                             Df Sum Sq Mean Sq   F value    Pr(>F)    \nasiadata2$infant_mortality    1  98874   98874 14217.304 < 2.2e-16 ***\nasiadata2$fertility           1    158     158    22.676 2.092e-06 ***\nasiadata2$region              4   2583     646    92.857 < 2.2e-16 ***\nasiadata2$gdppercap           1    837     837   120.331 < 2.2e-16 ***\nResiduals                  1590  11058       7                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere we can see that all the variables are significant. For detailed relation direction, we will see below.\n\nsummary(lm1)\n\n\nCall:\nlm(formula = asiadata2$life_expectancy ~ asiadata2$infant_mortality + \n    asiadata2$fertility + asiadata2$region + asiadata2$gdppercap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.930  -1.458   0.048   1.419   9.407 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                         7.660e+01  2.721e-01 281.482  < 2e-16 ***\nasiadata2$infant_mortality         -1.605e-01  2.528e-03 -63.485  < 2e-16 ***\nasiadata2$fertility                -5.181e-01  5.613e-02  -9.231  < 2e-16 ***\nasiadata2$regionEastern Asia       -2.196e+00  3.150e-01  -6.971 4.60e-12 ***\nasiadata2$regionSouth-Eastern Asia -1.355e+00  2.771e-01  -4.890 1.11e-06 ***\nasiadata2$regionSouthern Asia       2.651e-01  2.813e-01   0.942    0.346    \nasiadata2$regionWestern Asia        1.292e+00  2.765e-01   4.671 3.24e-06 ***\nasiadata2$gdppercap                 9.079e-05  8.277e-06  10.970  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.637 on 1590 degrees of freedom\nMultiple R-squared:  0.9026,    Adjusted R-squared:  0.9022 \nF-statistic:  2105 on 7 and 1590 DF,  p-value: < 2.2e-16\n\n\nHere we know infant mortality and fertility have negative influence on life time. Gdp per capital have positive influence on life time.\nThe life time in different regions are not all same.\n\n#save the output\noutcome <- broom::tidy(lm1)\n\nfuture work we can do\nDo imputation to make use of the entire data. Find the countries who have the top life time to find what’s their similarities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MADA2023 Data Analysis Portfolio",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio."
  },
  {
    "objectID": "dataanalysis_exercise.html",
    "href": "dataanalysis_exercise.html",
    "title": "Module 4 Data Analysis Exercise",
    "section": "",
    "text": "Summary\nData for this exercise was pulled from the CDC’s data repository. This dataset describes drug poisoning deaths at the county level by selected demographic characteristics and includes age-adjusted death rates for drug poisoning from 1999 to 2015.\nLoading packages, dataset, and reviewing the raw data:\n\n#load packages\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ dplyr   1.1.0\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ tidyr   1.3.0     ✔ forcats 1.0.0\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(naniar)\n\n#load dataset\ndrug_poison_data <- read_csv(\"dataanalysis-exercise/data/raw/drug_poison_data.csv\")\n\nRows: 53387 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): state, ST, county, death_rate\ndbl (4): FIPS, year, FIPS_state, population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndrug_poison_data <- read_csv(\"dataanalysis-exercise/data/raw_data/drug_poison_data.csv\")\n\nRows: 53387 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): state, ST, county, death_rate\ndbl (4): FIPS, year, FIPS_state, population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#examine data\nsummary(drug_poison_data)\n\n      FIPS            year         state                ST           \n Min.   : 1001   Min.   :1999   Length:53387       Length:53387      \n 1st Qu.:18181   1st Qu.:2003   Class :character   Class :character  \n Median :29179   Median :2007   Mode  :character   Mode  :character  \n Mean   :30411   Mean   :2007                                        \n 3rd Qu.:45083   3rd Qu.:2011                                        \n Max.   :56045   Max.   :2015                                        \n                                                                     \n   FIPS_state       county            population        death_rate       \n Min.   : 1.00   Length:53387       Min.   :      55   Length:53387      \n 1st Qu.:18.00   Class :character   1st Qu.:   11114   Class :character  \n Median :29.00   Mode  :character   Median :   25455   Mode  :character  \n Mean   :30.31                      Mean   :   95806                     \n 3rd Qu.:45.00                      3rd Qu.:   65101                     \n Max.   :56.00                      Max.   :10170292                     \n                                    NA's   :4                            \n\n\nCreating a new dataframe called “data” with variables of interest:\nThe selected variables here are year, state, county, population and death rate.\n\n#refined dataset with only variables of interest\ndata <- drug_poison_data %>%\n    select(year, state, county, population, death_rate)\n\nFilter to look at only cases in Georgia:\n\ngeorgia <- data %>%\n  filter(state %in% \"Georgia\")\n\nThis new final dataset (georgia) gives us cases of drug poisoning from 1999-2015 in the state of Georgia.\nSave cleaned data as an RDS file in processed_data folder:\n\nsaveRDS(georgia, file=\"dataanalysis-exercise/data/processed/georgia_clean.rds\")\nsaveRDS(georgia, file=\"dataanalysis-exercise/data/processed_data/georgia_clean.rds\")\n\nSave summary table:\n\n#make summary table\nsummary_georgia = data.frame(do.call(cbind, lapply(georgia, summary)))\nprint(summary_georgia)\n\n        year     state    county       population death_rate\nMin.    1999      2703      2703             1639       2703\n1st Qu. 2003 character character          11227.5  character\nMedian  2007 character character            22092  character\nMean    2007      2703      2703 58029.9470958195       2703\n3rd Qu. 2011 character character            47888  character\nMax.    2015 character character          1010562  character\n\n#save it as an RDS file\nsaveRDS(summary_georgia, file=\"dataanalysis-exercise/results/summary_georgia.rds\")\n\n—————————————\nTHIS SECTION ADDED BY NICOLE LUISI\n—————————————\n\nLoad required packages\n\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(dplyr)\n\n\n\nRead in clean RDS file and review\n\ngeorgia_clean_NL <- readRDS(here(\"dataanalysis-exercise\", \"data\", \"processed_data\", \"georgia_clean.rds\"))\n\nstr(georgia_clean_NL)\n\ntibble [2,703 × 5] (S3: tbl_df/tbl/data.frame)\n $ year      : num [1:2703] 2014 2011 2010 2015 2010 ...\n $ state     : chr [1:2703] \"Georgia\" \"Georgia\" \"Georgia\" \"Georgia\" ...\n $ county    : chr [1:2703] \"Union County, GA\" \"Walker County, GA\" \"Fulton County, GA\" \"Greene County, GA\" ...\n $ population: num [1:2703] 21952 68617 926038 16710 9093 ...\n $ death_rate: chr [1:2703] \"18.1-20\" \"14.1-16\" \"10.1-12\" \"10.1-12\" ...\n\nhead(georgia_clean_NL)\n\n# A tibble: 6 × 5\n   year state   county                population death_rate\n  <dbl> <chr>   <chr>                      <dbl> <chr>     \n1  2014 Georgia Union County, GA           21952 18.1-20   \n2  2011 Georgia Walker County, GA          68617 14.1-16   \n3  2010 Georgia Fulton County, GA         926038 10.1-12   \n4  2015 Georgia Greene County, GA          16710 10.1-12   \n5  2010 Georgia Montgomery County, GA       9093 8.1-10    \n6  2012 Georgia Worth County, GA           21464 8.1-10    \n\n\n\n\nCan’t work with death_rate variable as character range so splitting into numeric min and max variables\n\ngeorgia_clean_NL$death_rate_min <- as.numeric(str_extract(georgia_clean_NL$death_rate, \"[^-]+\"))\n\nWarning: NAs introduced by coercion\n\ngeorgia_clean_NL$death_rate_max <- as.numeric(str_extract(georgia_clean_NL$death_rate, '\\\\b\\\\w+$'))\n\n# Check min and max\nhead(georgia_clean_NL[,c(\"death_rate\", \"death_rate_min\", \"death_rate_max\")])\n\n# A tibble: 6 × 3\n  death_rate death_rate_min death_rate_max\n  <chr>               <dbl>          <dbl>\n1 18.1-20              18.1             20\n2 14.1-16              14.1             16\n3 10.1-12              10.1             12\n4 10.1-12              10.1             12\n5 8.1-10                8.1             10\n6 8.1-10                8.1             10\n\n\n\n\nCreate subsets for Dekalb and Fulton county\n\ngeorgia_clean_NL_dekalb <- georgia_clean_NL %>%\n  filter(county == \"DeKalb County, GA\")\ngeorgia_clean_NL_dekalb <- georgia_clean_NL_dekalb[,c(\"year\", \"county\",\"death_rate_min\", \"death_rate_max\")]\ngeorgia_clean_NL_fulton <- georgia_clean_NL %>%\n  filter(county == \"Fulton County, GA\")\ngeorgia_clean_NL_fulton <- georgia_clean_NL_fulton[,c(\"year\", \"county\",\"death_rate_min\", \"death_rate_max\")]\n\n\n\nPlot min and max death rates for Dekalb and Fulton county\n\n# Transform to long format\ngeorgia_clean_NL_dekalb_long <- gather(georgia_clean_NL_dekalb, group, value, death_rate_min:death_rate_max) %>% arrange(factor(year, levels = c(\"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"))) %>% \n  mutate(year=factor(year, levels=unique(year)))\n\ngeorgia_clean_NL_fulton_long <- gather(georgia_clean_NL_fulton, group, value, death_rate_min:death_rate_max) %>% arrange(factor(year, levels = c(\"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"))) %>% \n  mutate(year=factor(year, levels=unique(year)))\n\n# Dekalb plot\ndekalb_plot <- ggplot(georgia_clean_NL_dekalb_long, aes(x = year, y = value, fill = group)) +\n  geom_col(position = \"dodge\", colour = \"black\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  ylab(\"Year\") +\n  xlab(\"Age-Adjusted Death Rate for Drug Poisoning\") +  \n  ggtitle(\"Dekalb County\") + \n  scale_y_continuous(breaks=seq(0, 15, 5))\n\n# Fulton plot\nfulton_plot <- ggplot(georgia_clean_NL_fulton_long, aes(x = year, y = value, fill = group)) +\n  geom_col(position = \"dodge\", colour = \"black\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  ylab(\"Year\") +\n  xlab(\"Age-Adjusted Death Rate for Drug Poisoning\") +  \n  ggtitle(\"Fulton County\") + \n  scale_y_continuous(breaks=seq(0, 15, 15))\n\n# Grid of plots\ngrid.arrange(dekalb_plot, fulton_plot, ncol = 1)\n\n\n\n\n\n\nCreate objects with overall min and max by year for 2015 and 2000\n\nyear_minmax <- georgia_clean_NL %>% select(year, death_rate_min, death_rate_max)\n\nyear_minmax1 <- year_minmax %>%\n  group_by(year) %>%\n  summarise(\n    MaxByYear = max(death_rate_max, na.rm = T),\n    MinByYear = min(death_rate_min, na.rm = T)\n  ) %>%\n  arrange(year)\n\n\n\nPlot overall (lowest) min and (highest) max death rate by year\n\nplot(year_minmax1$year,                             \n     year_minmax1$MinByYear,\n     main = \"Overall Min and Max Death Rate by Year, All Counties\",\n     type = \"l\",\n     col = 2,\n     ylim = c(- 15, 40),\n     xlab = \"Year\",\n     ylab = \"Death Rate\")\nlines(year_minmax1$year,                            \n      year_minmax1$MaxByYear,\n      type = \"l\",\n      col = 3)\nlegend(\"topright\",                          \n       c(\"Min Death Rate\", \"Max Death Rate\"),\n       lty = 1,\n       col = 2:4)"
  },
  {
    "objectID": "fluanalysis/code/exploration.html",
    "href": "fluanalysis/code/exploration.html",
    "title": "Module 8 Exploration",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nAs part of the exploratory analysis, this code produces plots, tables, and other summary quantities for some interesting aspects of our data\n\nLoad packages\n\nlibrary(tidyverse) #for transforming data\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(here) #to set paths for data loading/saving\n\nhere() starts at /Users/irenecavros/Desktop/UGA/2022-2023/Spring 2023/MADA 2023/Github/irenecavros-mada-portfolio\n\nlibrary(dplyr) #for data processing/cleaning\nlibrary(tidyr) #for data processing/cleaning\nlibrary(gtsummary) #for summarizing data\nlibrary(forcats) #for working with categorical variables\nlibrary(ggplot2) #for nice plots\nlibrary(cowplot) #addon to ggplot for nice plots\nlibrary(skimr) #for nice visualization of data \nlibrary(tidymodels) #for modeling\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.3     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.4     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ recipes::all_double()  masks gtsummary::all_double()\n✖ recipes::all_factor()  masks gtsummary::all_factor()\n✖ recipes::all_integer() masks gtsummary::all_integer()\n✖ recipes::all_logical() masks gtsummary::all_logical()\n✖ recipes::all_numeric() masks gtsummary::all_numeric()\n✖ scales::discard()      masks purrr::discard()\n✖ dplyr::filter()        masks stats::filter()\n✖ recipes::fixed()       masks stringr::fixed()\n✖ dplyr::lag()           masks stats::lag()\n✖ yardstick::spec()      masks readr::spec()\n✖ recipes::step()        masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\n\n####Load the data\n\n#Path to data. Note the use of the here() package and not absolute paths\ncleandata <- readRDS(here(\"fluanalysis\", \"data\", \"cleandata.rds\"))\n\n\n\nLook at variable types\n\nstr(cleandata)\n\n'data.frame':   730 obs. of  32 variables:\n $ SwollenLymphNodes: Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 1 1 2 1 ...\n $ ChestCongestion  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 2 1 1 1 2 2 2 ...\n $ ChillsSweats     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 2 2 2 1 ...\n $ NasalCongestion  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 2 1 1 1 2 2 2 ...\n $ CoughYN          : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 1 2 2 2 2 2 ...\n $ Sneeze           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 1 2 1 2 1 1 ...\n $ Fatigue          : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ SubjectiveFever  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 1 ...\n $ Headache         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 1 2 2 2 ...\n $ Weakness         : Factor w/ 4 levels \"None\",\"Mild\",..: 2 4 4 4 3 3 2 4 3 3 ...\n $ WeaknessYN       : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ CoughIntensity   : Factor w/ 4 levels \"None\",\"Mild\",..: 4 4 2 3 1 3 4 3 3 3 ...\n $ CoughYN2         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 2 2 2 2 ...\n $ Myalgia          : Factor w/ 4 levels \"None\",\"Mild\",..: 2 4 4 4 2 3 2 4 3 2 ...\n $ MyalgiaYN        : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ RunnyNose        : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 1 1 2 2 2 2 ...\n $ AbPain           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n $ ChestPain        : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 2 2 1 1 1 ...\n $ Diarrhea         : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 2 1 1 1 1 ...\n $ EyePn            : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 2 1 1 1 1 1 ...\n $ Insomnia         : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 1 1 2 2 2 ...\n $ ItchyEye         : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Nausea           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 2 ...\n $ EarPn            : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 1 1 1 1 1 1 ...\n $ Hearing          : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 1 1 1 1 1 ...\n $ Pharyngitis      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 1 1 1 ...\n $ Breathless       : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 2 1 1 1 2 ...\n $ ToothPn          : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 2 1 ...\n $ Vision           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Vomit            : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n $ Wheeze           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 2 1 1 1 1 ...\n $ BodyTemp         : num  98.3 100.4 100.8 98.8 100.5 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:5] 133 243 363 577 585\n  ..- attr(*, \"names\")= chr [1:5] \"133\" \"243\" \"363\" \"577\" ...\n\n\n\n\nProduce and save a summary table.\nThis also helps us get things into the shape of a data frame (for easier saving)\n\n# looking at the data\nsummary_df <- dplyr::glimpse(cleandata)\n\nRows: 730\nColumns: 32\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y…\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, …\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ CoughYN           <fct> Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, …\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, …\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes…\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes…\n$ Weakness          <fct> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi…\n$ WeaknessYN        <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ CoughIntensity    <fct> Severe, Severe, Mild, Moderate, None, Moderate, Seve…\n$ CoughYN2          <fct> Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes…\n$ Myalgia           <fct> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, …\n$ MyalgiaYN         <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No…\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N…\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, …\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,…\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye…\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y…\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,…\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y…\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y…\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,…\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, …\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, …\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N…\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N…\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N…\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, …\n\n# save to file\nsummarytable_file = here(\"fluanalysis\", \"data\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\n\n\nOutcomes of interest\nFor our further analysis, we decide that our main continuous outcome of interest is Body temperature, and our main categorical outcome is Nausea. We want to see if the other symptoms are correlated with (predict) those outcomes.\n\nsummary(cleandata)\n\n SwollenLymphNodes ChestCongestion ChillsSweats NasalCongestion CoughYN  \n No :418           No :323         No :130      No :167         No : 75  \n Yes:312           Yes:407         Yes:600      Yes:563         Yes:655  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Sneeze    Fatigue   SubjectiveFever Headache      Weakness   WeaknessYN\n No :339   No : 64   No :230         No :115   None    : 49   No : 49   \n Yes:391   Yes:666   Yes:500         Yes:615   Mild    :223   Yes:681   \n                                               Moderate:338             \n                                               Severe  :120             \n                                                                        \n                                                                        \n  CoughIntensity CoughYN2      Myalgia    MyalgiaYN RunnyNose AbPain   \n None    : 47    No : 47   None    : 79   No : 79   No :211   No :639  \n Mild    :154    Yes:683   Mild    :213   Yes:651   Yes:519   Yes: 91  \n Moderate:357              Moderate:325                                \n Severe  :172              Severe  :113                                \n                                                                       \n                                                                       \n ChestPain Diarrhea  EyePn     Insomnia  ItchyEye  Nausea    EarPn    \n No :497   No :631   No :617   No :315   No :551   No :475   No :568  \n Yes:233   Yes: 99   Yes:113   Yes:415   Yes:179   Yes:255   Yes:162  \n                                                                      \n                                                                      \n                                                                      \n                                                                      \n Hearing   Pharyngitis Breathless ToothPn   Vision    Vomit     Wheeze   \n No :700   No :119     No :436    No :565   No :711   No :652   No :510  \n Yes: 30   Yes:611     Yes:294    Yes:165   Yes: 19   Yes: 78   Yes:220  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n    BodyTemp     \n Min.   : 97.20  \n 1st Qu.: 98.20  \n Median : 98.50  \n Mean   : 98.94  \n 3rd Qu.: 99.30  \n Max.   :103.10  \n\n\nLet’s produce and print some summary output with a smaller subset of variables.\n\n# make subset with a few variables to summarize\nsubset <- cleandata %>% select(BodyTemp, SwollenLymphNodes, ChillsSweats, AbPain, Diarrhea, Nausea, Vomit)\n\n# summarize the data\nsubsettable <- \n  tbl_summary(subset)\nsubsettable\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      N = 7301\n    \n  \n  \n    BodyTemp\n98.50 (98.20, 99.30)\n    SwollenLymphNodes\n312 (43%)\n    ChillsSweats\n600 (82%)\n    AbPain\n91 (12%)\n    Diarrhea\n99 (14%)\n    Nausea\n255 (35%)\n    Vomit\n78 (11%)\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\n\n\nOutcome of interest #1: Body temperature\nSince body temperature is continuous, let’s create a histogram/density plot.\n\np1 <- ggplot(subset, aes(BodyTemp)) + \n  geom_density() + \n  labs(title = \"Body Temperature Density\", x = \"Body Temperature (Farenheit)\", y = \"Density\") + scale_fill_discrete(labels=c('No', 'Yes')) + theme_classic()\n\np1\n\n\n\n\nThis tells us the frequency of body temperatures in the dataset.\nLet’s create some boxplots including this outcome and the shortlist of independent variables/predictors we have chosen.\n\n#boxplot of BodyTemp x SwollenLymphNodes\np2 <- ggplot(subset, aes(x=BodyTemp, y=SwollenLymphNodes)) + \n  geom_boxplot(fill = \"gray\") +\n  labs(title = \"BodyTemp x SwollenLymphNodes\", x = \"Body Temperature (Farenheit)\", y = \"Swollen Lymph Nodes\") + scale_fill_discrete(labels=c('No', 'Yes')) + theme_classic()\n\np2\n\n\n\n\nBased on this plot, it looks like the average body temperature among those without swollen lymph nodes looks marginally higher with a larger spread. This doesn’t really point to anything too interesting so let’s try chills.\n\n#boxplot of BodyTemp x ChillsSweats\np3 <- ggplot(subset, aes(x=BodyTemp, y=ChillsSweats)) + \n  geom_boxplot(fill = \"gray\") +\n  labs(title = \"BodyTemp x ChillsSweats\", x = \"Body Temperature (Farenheit)\", y = \"Experienced Chills or Sweats\") + scale_fill_discrete(labels=c('No', 'Yes')) + theme_classic()\n\np3\n\n\n\n\nIn this plot, the mean body temperature among those who experienced chills or sweating appears about a half degree higher than those without, and the spread is larger as well. This could potentially be interesting, but let’s do a few more plots to look at the nausea outcome as well.\nFor now, let’s use nausea as a predictor and keep bodytemp as the outcome.\n\n#boxplot of BodyTemp x Nausea\np4 <- ggplot(subset, aes(x=BodyTemp, y=Nausea)) + \n  geom_boxplot(fill = \"gray\") +\n  labs(title = \"BodyTemp x Nausea\", x = \"Body Temperature (Farenheit)\", y = \"Nausea\") + scale_fill_discrete(labels=c('No', 'Yes')) + theme_classic()\n\np4\n\n\n\n\nNothing appears too interesting here.\n\n\nOutcome of interest #2: Nausea\nNext let’s make nausea the outcome of interest. We can create some plots for this outcome and a few independent variables/predictors.\n\n#plot of Nausea x AbPain\nNauAb <- data.frame(table(subset$Nausea,subset$AbPain))\nnames(NauAb) <- c(\"Nausea\",\"AbPain\",\"Count\")\n\np5<- ggplot(data=NauAb, aes(x=Nausea, y=Count, fill=AbPain)) + geom_bar(stat=\"identity\")\np5\n\n\n\n\nAbdominal pain looks to be more common in those with nausea, than those without, which makes sense!\n\n#plot of Nausea x Diarrhea\nNauDi <- data.frame(table(subset$Nausea,subset$Diarrhea))\nnames(NauDi) <- c(\"Nausea\",\"Diarrhea\",\"Count\")\n\np6<- ggplot(data=NauDi, aes(x=Nausea, y=Count, fill=Diarrhea)) + geom_bar(stat=\"identity\")\np6\n\n\n\n\nDiarrhea also appears to be more frequent among those with nausea, which, again, is logical.\n\n#plot of Nausea x Vomit\nNauVom <- data.frame(table(subset$Nausea,subset$Vomit))\nnames(NauVom) <- c(\"Nausea\",\"Vomit\",\"Count\")\n\np7<- ggplot(data=NauVom, aes(x=Vomit, y=Count, fill=Nausea)) + geom_bar(stat=\"identity\")\np7\n\n\n\n\nThis time we changed the X axis so we are now looking at among those who vomited, how many had nausea and among those who did not vomit, how many had nausea? A large proportion of those who vomited also had nausea, which also makes sense."
  },
  {
    "objectID": "fluanalysis/code/wrangling.html",
    "href": "fluanalysis/code/wrangling.html",
    "title": "Module 8 Wrangling",
    "section": "",
    "text": "Introduction\nDuring the exercise for model 8, we will practice a bit of model fitting using the tidymodels framework.\n\n\nLoad packages\n\nlibrary('tidyverse') #for transforming data\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary('dplyr') #for data processing/cleaning\nlibrary('tidyr') #for data processing/cleaning\nlibrary('skimr') #for nice visualization of data \nlibrary('here') #to set paths\n\nhere() starts at /Users/irenecavros/Desktop/UGA/2022-2023/Spring 2023/MADA 2023/Github/irenecavros-mada-portfolio\n\n\n\n\nGet the data\nNote: the file we want to work with is actually an RDS file/object, not an RData/RDA file, despite the somewhat misleading .Rda file name ending. So you need to use the right function to load RDS files into R.\n\ndata <- readRDS(here(\"fluanalysis\", \"data\", \"SympAct_Any_Pos.Rda\"))\n\n\n\nRemove all variables that have Score or Total or FluA or FluB or Dxname or Activity in their name.\nDon’t do this manually one by one, figure out how to use R commands that let you remove things in an efficient manner, using e.g., contains() or starts_with() from dplyr/tidyselect\nAlso remove the variable Unique.Visit.\n\ndata <- as.data.frame(data)\ndf2 <- data %>% \n    select(-contains('Score')) %>% #to remove variables containing xyz\n    select(-contains('Total')) %>%\n    select(-contains('FluA')) %>%\n    select(-contains('FluB')) %>%\n    select(-contains('Dxname')) %>%\n    select(-contains('Activity')) %>%\n    select(-c('Unique.Visit')) #to remove final variable \n\nWe are now left with 32 variables coding for presence or absence of some symptom. Only one, temperature, is continuous. A few have multiple categories.\n\n\nLet’s remove any NA observations\nThere aren’t many.\n\ncleandata<- na.omit(df2)\n\n\n\nSaving the cleaned data\n\nsave_data_location <- here::here(\"fluanalysis\", \"data\", \"cleandata.rds\")\nsaveRDS(cleandata, file = save_data_location)"
  },
  {
    "objectID": "fluanalysis/code/fitting.html",
    "href": "fluanalysis/code/fitting.html",
    "title": "Module 8 Fitting",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nThe code in this file fits some models to the flu data we have.\n\nLoad packages\n\n# Helper packages\nlibrary(tidyverse) #for transforming data\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)     # for data wrangling\nlibrary(tidyr)     # for data wrangling\nlibrary(ggplot2)   # for awesome plotting\nlibrary(here)      # to set paths for data loading/saving\n\nhere() starts at /Users/irenecavros/Desktop/UGA/2022-2023/Spring 2023/MADA 2023/Github/irenecavros-mada-portfolio\n\n# Modeling packages\nlibrary(tidymodels) #for modeling\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.3     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.4     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(performance) # to look at model performance\n\n\nAttaching package: 'performance'\n\nThe following objects are masked from 'package:yardstick':\n\n    mae, rmse\n\n# Model interpretability packages\nlibrary(vip)       # variable importance\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\n\n1. Load clean data\n\n#Path to data. Note the use of the here() package and not absolute paths\ncleandata <- readRDS(here(\"fluanalysis\", \"data\", \"cleandata.rds\"))\n\n\n\n2. Fit a linear model to the continuous outcome (Body temperature) using only the main predictor of interest\nLet’s set up our model. For linear modeling we’ll need to use the linear_reg() and set_engine(“lm”) commands.\n\nlm_model2 <- linear_reg() %>%\n  set_engine(\"lm\")\n\nNow let’s train it to our data\n\nlm_fit2 <- lm_model2 %>% \n          fit(BodyTemp ~ RunnyNose, data = cleandata)\nlm_fit2\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ RunnyNose, data = data)\n\nCoefficients:\n (Intercept)  RunnyNoseYes  \n     99.1431       -0.2926  \n\n\nReport results using the glance() function.\n\nglance(lm_fit2)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0123  0.0110  1.19    9.08 0.00268     1 -1162. 2329. 2343.   1031.     728\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\nAdditional summary data\n\ntidy(lm_fit2)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.1      0.0819   1210.   0      \n2 RunnyNoseYes   -0.293    0.0971     -3.01 0.00268\n\n\nChecking the model performance\n\ncheck_model(lm_fit2$fit)\n\n\n\n\n\n\n3. Fit another linear model to the continuous outcome using all predictors of interest\nLet’s set up a second model with all predictors. Like before, we’ll need to use the linear_reg() and set_engine(“lm”) commands.\n\nlm_model3 <- linear_reg() %>%\n  set_engine(\"lm\")\n\nNow let’s train it to our data\n\nlm_fit3 <- lm_model3 %>% \n          fit(BodyTemp ~ ., data = cleandata)\nlm_fit3\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ ., data = data)\n\nCoefficients:\n           (Intercept)    SwollenLymphNodesYes      ChestCongestionYes  \n             97.925243               -0.165302                0.087326  \n       ChillsSweatsYes      NasalCongestionYes              CoughYNYes  \n              0.201266               -0.215771                0.313893  \n             SneezeYes              FatigueYes      SubjectiveFeverYes  \n             -0.361924                0.264762                0.436837  \n           HeadacheYes            WeaknessMild        WeaknessModerate  \n              0.011453                0.018229                0.098944  \n        WeaknessSevere           WeaknessYNYes      CoughIntensityMild  \n              0.373435                      NA                0.084881  \nCoughIntensityModerate    CoughIntensitySevere             CoughYN2Yes  \n             -0.061384               -0.037272                      NA  \n           MyalgiaMild         MyalgiaModerate           MyalgiaSevere  \n              0.164242               -0.024064               -0.129263  \n          MyalgiaYNYes            RunnyNoseYes               AbPainYes  \n                    NA               -0.080485                0.031574  \n          ChestPainYes             DiarrheaYes                EyePnYes  \n              0.105071               -0.156806                0.131544  \n           InsomniaYes             ItchyEyeYes               NauseaYes  \n             -0.006824               -0.008016               -0.034066  \n              EarPnYes              HearingYes          PharyngitisYes  \n              0.093790                0.232203                0.317581  \n         BreathlessYes              ToothPnYes               VisionYes  \n              0.090526               -0.022876               -0.274625  \n              VomitYes               WheezeYes  \n              0.165272               -0.046665  \n\n\nWe can again look at results using the glance() function.\n\nglance(lm_fit3)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.129  0.0860  1.14    3.02 4.20e-8    34 -1116. 2304. 2469.    909.     695\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\nAdditional summary data\n\ntidy(lm_fit3)\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic   p.value\n   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)           97.9       0.304   322.     0        \n 2 SwollenLymphNodesYes  -0.165     0.0920   -1.80   0.0727   \n 3 ChestCongestionYes     0.0873    0.0975    0.895  0.371    \n 4 ChillsSweatsYes        0.201     0.127     1.58   0.114    \n 5 NasalCongestionYes    -0.216     0.114    -1.90   0.0584   \n 6 CoughYNYes             0.314     0.241     1.30   0.193    \n 7 SneezeYes             -0.362     0.0983   -3.68   0.000249 \n 8 FatigueYes             0.265     0.161     1.65   0.0996   \n 9 SubjectiveFeverYes     0.437     0.103     4.22   0.0000271\n10 HeadacheYes            0.0115    0.125     0.0913 0.927    \n# … with 28 more rows\n\n\nCheck model performance\n\ncheck_model(lm_fit3$fit)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n4. Compare the model results for the model with just the main predictor and all predictors\nComparison of lm_fit2 and lm_fit3 based on performance of the models\n\ncompare_performance(lm_fit2,lm_fit3)\n\n# Comparison of Model Performance Indices\n\nName    | Model |  AIC (weights) | AICc (weights) |  BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------------------------------------------\nlm_fit2 |   _lm | 2329.3 (<.001) | 2329.4 (<.001) | 2343.1 (>.999) | 0.012 |     0.011 | 1.188 | 1.190\nlm_fit3 |   _lm | 2303.8 (>.999) | 2307.7 (>.999) | 2469.2 (<.001) | 0.129 |     0.086 | 1.116 | 1.144\n\n\n\n\n5. Fit a logistic model to the categorical outcome (Nausea) using only the main predictor of interest\nFor logistic modeling, we will need to use logistic_reg() and set_engine(“glm”).\n\nglm_model5 <- logistic_reg() %>%\n  set_engine(\"glm\")\n\nNow let’s train it to our data\n\nglm_fit5 <- glm_model5 %>% \n          fit(Nausea ~ RunnyNose, data = cleandata)\nglm_fit5\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Nausea ~ RunnyNose, family = stats::binomial, \n    data = data)\n\nCoefficients:\n (Intercept)  RunnyNoseYes  \n    -0.65781       0.05018  \n\nDegrees of Freedom: 729 Total (i.e. Null);  728 Residual\nNull Deviance:      944.7 \nResidual Deviance: 944.6    AIC: 948.6\n\n\nReport results using the glance() function.\n\nglance(glm_fit5)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -472.  949.  958.     945.         728   730\n\n\nAdditional summary data\n\ntidy(glm_fit5)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic    p.value\n  <chr>           <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   -0.658      0.145    -4.53  0.00000589\n2 RunnyNoseYes   0.0502     0.172     0.292 0.770     \n\n\nCheck model performance\n\ncheck_model(glm_fit5$fit)\n\n\n\n\n\n\n6. Fit another logistic model to the categorical outcome using all (important) predictors of interest\nLet’s set up a second model with all predictors. Like before, we’ll need to use the logistic_reg() and set_engine(“glm”) commands.\n\nglm_model6 <- logistic_reg() %>%\n  set_engine(\"glm\")\n\nNow let’s train it to our data\n\nglm_fit6 <- glm_model6 %>% \n          fit(Nausea ~ ., data = cleandata)\nglm_fit6\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Nausea ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n           (Intercept)    SwollenLymphNodesYes      ChestCongestionYes  \n              0.222870               -0.251083                0.275554  \n       ChillsSweatsYes      NasalCongestionYes              CoughYNYes  \n              0.274097                0.425817               -0.140423  \n             SneezeYes              FatigueYes      SubjectiveFeverYes  \n              0.176724                0.229062                0.277741  \n           HeadacheYes            WeaknessMild        WeaknessModerate  \n              0.331259               -0.121606                0.310849  \n        WeaknessSevere           WeaknessYNYes      CoughIntensityMild  \n              0.823187                      NA               -0.220794  \nCoughIntensityModerate    CoughIntensitySevere             CoughYN2Yes  \n             -0.362678               -0.950544                      NA  \n           MyalgiaMild         MyalgiaModerate           MyalgiaSevere  \n             -0.004146                0.204743                0.120758  \n          MyalgiaYNYes            RunnyNoseYes               AbPainYes  \n                    NA                0.045324                0.939304  \n          ChestPainYes             DiarrheaYes                EyePnYes  \n              0.070777                1.063934               -0.341991  \n           InsomniaYes             ItchyEyeYes                EarPnYes  \n              0.084175               -0.063364               -0.181719  \n            HearingYes          PharyngitisYes           BreathlessYes  \n              0.323052                0.275364                0.526801  \n            ToothPnYes               VisionYes                VomitYes  \n              0.480649                0.125498                2.458466  \n             WheezeYes                BodyTemp  \n             -0.304435               -0.031246  \n\nDegrees of Freedom: 729 Total (i.e. Null);  695 Residual\nNull Deviance:      944.7 \nResidual Deviance: 751.5    AIC: 821.5\n\n\nReport results using the glance() function.\n\nglance(glm_fit6)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -376.  821.  982.     751.         695   730\n\n\nAdditional summary data\n\ntidy(glm_fit6)\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)             0.223     7.83     0.0285  0.977 \n 2 SwollenLymphNodesYes   -0.251     0.196   -1.28    0.200 \n 3 ChestCongestionYes      0.276     0.213    1.30    0.195 \n 4 ChillsSweatsYes         0.274     0.288    0.952   0.341 \n 5 NasalCongestionYes      0.426     0.255    1.67    0.0944\n 6 CoughYNYes             -0.140     0.519   -0.271   0.787 \n 7 SneezeYes               0.177     0.210    0.840   0.401 \n 8 FatigueYes              0.229     0.372    0.616   0.538 \n 9 SubjectiveFeverYes      0.278     0.225    1.23    0.218 \n10 HeadacheYes             0.331     0.285    1.16    0.245 \n# … with 28 more rows\n\n\nCheck model performance\n\ncheck_model(glm_fit6$fit)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n7. Compare the model results for the categorical model with just the main predictor and all predictors\nComparison of glm_fit5 and glm_fit6 based on performance of the models\n\ncompare_performance(glm_fit5, glm_fit6)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\n# Comparison of Model Performance Indices\n\nName     | Model | AIC (weights) | AICc (weights) | BIC (weights) | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------------------------------------------------------\nglm_fit5 |  _glm | 948.6 (<.001) |  948.6 (<.001) | 957.8 (>.999) | 1.169e-04 | 0.477 | 1.139 |    0.647 |  -107.871 |           0.012 | 0.545\nglm_fit6 |  _glm | 821.5 (>.999) |  825.1 (>.999) | 982.2 (<.001) |     0.247 | 0.414 | 1.040 |    0.515 |      -Inf |           0.002 | 0.658"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "Module Evaluation",
    "section": "",
    "text": "Load Packages\n\nlibrary(here) #to set paths for data \n\nhere() starts at /Users/irenecavros/Desktop/UGA/2022-2023/Spring 2023/MADA 2023/Github/irenecavros-mada-portfolio\n\nlibrary(skimr) #for summarizing data\nlibrary(tidyverse) #for data processing\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels) #for modeling\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.3     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.4     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(glmnet) #for modeling\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-6\n\nlibrary(performance) #for modeling\n\n\nAttaching package: 'performance'\n\nThe following objects are masked from 'package:yardstick':\n\n    mae, rmse\n\n\n\n\nLoad Dataset\n\n#Path to data. \ndata_location <- here::here(\"fluanalysis\",\"data\",\"cleandata.rds\")\n#load data\nmydata <- readRDS(data_location)\n\n\n\nData Summary\nLet’s take a high-level look at the data before we start.\n\nhead(mydata)\n\n  SwollenLymphNodes ChestCongestion ChillsSweats NasalCongestion CoughYN Sneeze\n1               Yes              No           No              No     Yes     No\n2               Yes             Yes           No             Yes     Yes     No\n3               Yes             Yes          Yes             Yes      No    Yes\n4               Yes             Yes          Yes             Yes     Yes    Yes\n5               Yes              No          Yes              No      No     No\n6                No              No          Yes              No     Yes    Yes\n  Fatigue SubjectiveFever Headache Weakness WeaknessYN CoughIntensity CoughYN2\n1     Yes             Yes      Yes     Mild        Yes         Severe      Yes\n2     Yes             Yes      Yes   Severe        Yes         Severe      Yes\n3     Yes             Yes      Yes   Severe        Yes           Mild      Yes\n4     Yes             Yes      Yes   Severe        Yes       Moderate      Yes\n5     Yes             Yes      Yes Moderate        Yes           None       No\n6     Yes             Yes      Yes Moderate        Yes       Moderate      Yes\n   Myalgia MyalgiaYN RunnyNose AbPain ChestPain Diarrhea EyePn Insomnia\n1     Mild       Yes        No     No        No       No    No       No\n2   Severe       Yes        No     No        No       No    No       No\n3   Severe       Yes       Yes    Yes       Yes       No    No      Yes\n4   Severe       Yes       Yes     No        No       No    No      Yes\n5     Mild       Yes        No     No        No       No   Yes      Yes\n6 Moderate       Yes        No     No       Yes      Yes    No       No\n  ItchyEye Nausea EarPn Hearing Pharyngitis Breathless ToothPn Vision Vomit\n1       No     No    No      No         Yes         No      No     No    No\n2       No     No   Yes     Yes         Yes         No      No     No    No\n3       No    Yes    No      No         Yes        Yes     Yes     No    No\n4       No    Yes   Yes      No         Yes         No      No     No    No\n5       No    Yes    No      No         Yes         No      No     No    No\n6       No    Yes    No      No         Yes        Yes      No     No    No\n  Wheeze BodyTemp\n1     No     98.3\n2     No    100.4\n3     No    100.8\n4    Yes     98.8\n5     No    100.5\n6    Yes     98.4\n\nskim(mydata)\n\n\nData summary\n\n\nName\nmydata\n\n\nNumber of rows\n730\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSwollenLymphNodes\n0\n1\nFALSE\n2\nNo: 418, Yes: 312\n\n\nChestCongestion\n0\n1\nFALSE\n2\nYes: 407, No: 323\n\n\nChillsSweats\n0\n1\nFALSE\n2\nYes: 600, No: 130\n\n\nNasalCongestion\n0\n1\nFALSE\n2\nYes: 563, No: 167\n\n\nCoughYN\n0\n1\nFALSE\n2\nYes: 655, No: 75\n\n\nSneeze\n0\n1\nFALSE\n2\nYes: 391, No: 339\n\n\nFatigue\n0\n1\nFALSE\n2\nYes: 666, No: 64\n\n\nSubjectiveFever\n0\n1\nFALSE\n2\nYes: 500, No: 230\n\n\nHeadache\n0\n1\nFALSE\n2\nYes: 615, No: 115\n\n\nWeakness\n0\n1\nFALSE\n4\nMod: 338, Mil: 223, Sev: 120, Non: 49\n\n\nWeaknessYN\n0\n1\nFALSE\n2\nYes: 681, No: 49\n\n\nCoughIntensity\n0\n1\nFALSE\n4\nMod: 357, Sev: 172, Mil: 154, Non: 47\n\n\nCoughYN2\n0\n1\nFALSE\n2\nYes: 683, No: 47\n\n\nMyalgia\n0\n1\nFALSE\n4\nMod: 325, Mil: 213, Sev: 113, Non: 79\n\n\nMyalgiaYN\n0\n1\nFALSE\n2\nYes: 651, No: 79\n\n\nRunnyNose\n0\n1\nFALSE\n2\nYes: 519, No: 211\n\n\nAbPain\n0\n1\nFALSE\n2\nNo: 639, Yes: 91\n\n\nChestPain\n0\n1\nFALSE\n2\nNo: 497, Yes: 233\n\n\nDiarrhea\n0\n1\nFALSE\n2\nNo: 631, Yes: 99\n\n\nEyePn\n0\n1\nFALSE\n2\nNo: 617, Yes: 113\n\n\nInsomnia\n0\n1\nFALSE\n2\nYes: 415, No: 315\n\n\nItchyEye\n0\n1\nFALSE\n2\nNo: 551, Yes: 179\n\n\nNausea\n0\n1\nFALSE\n2\nNo: 475, Yes: 255\n\n\nEarPn\n0\n1\nFALSE\n2\nNo: 568, Yes: 162\n\n\nHearing\n0\n1\nFALSE\n2\nNo: 700, Yes: 30\n\n\nPharyngitis\n0\n1\nFALSE\n2\nYes: 611, No: 119\n\n\nBreathless\n0\n1\nFALSE\n2\nNo: 436, Yes: 294\n\n\nToothPn\n0\n1\nFALSE\n2\nNo: 565, Yes: 165\n\n\nVision\n0\n1\nFALSE\n2\nNo: 711, Yes: 19\n\n\nVomit\n0\n1\nFALSE\n2\nNo: 652, Yes: 78\n\n\nWheeze\n0\n1\nFALSE\n2\nNo: 510, Yes: 220\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nBodyTemp\n0\n1\n98.94\n1.2\n97.2\n98.2\n98.5\n99.3\n103.1\n▇▇▂▁▁\n\n\n\n\n\n\n\nSplitting data\nIf nausea is our main categorical outcome, we can split data with the Nausea outcome evenly into testing and training datasets.\n\nset.seed(123)\nnausea_split=initial_split(mydata,strata = Nausea)\nnausea_train=training(nausea_split)\nnausea_test=testing(nausea_split)\n\n\n\nModel 1 Evaluation\nLet’s first use a tidymodels recipe to create a workflow that fits a logistic model using all predictors.\n\nnausea_rec=recipe(Nausea~.,data=nausea_train)\nlr_mod=logistic_reg()%>%\n  set_engine(\"glm\")\nnausea_workflow=workflow()%>%\n  add_model(lr_mod)%>%\n  add_recipe(nausea_rec)\nnausea_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\nnausea_fit=nausea_workflow%>%\n  fit(data=nausea_train)\nnausea_fit%>%\n  extract_fit_parsnip()%>%\n  tidy()\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)           2.11        9.41    0.224     0.823\n 2 SwollenLymphNodesYes  0.0149      0.227   0.0658    0.948\n 3 ChestCongestionYes    0.327       0.246   1.33      0.183\n 4 ChillsSweatsYes       0.00322     0.332   0.00970   0.992\n 5 NasalCongestionYes    0.418       0.294   1.42      0.155\n 6 CoughYNYes           -0.217       0.629  -0.345     0.730\n 7 SneezeYes             0.185       0.247   0.750     0.453\n 8 FatigueYes            0.256       0.438   0.585     0.559\n 9 SubjectiveFeverYes    0.323       0.266   1.22      0.224\n10 HeadacheYes           0.00580     0.311   0.0186    0.985\n# … with 28 more rows\n\n\nNow let’s look at the predictions, ROC curve, and ROC-AUC for the data.\n\npredict(nausea_fit,nausea_train)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 547 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# … with 537 more rows\n\nnausea_aug_train=augment(nausea_fit,nausea_train)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nnausea_aug_train%>%\n  roc_curve(truth=Nausea,.pred_No)%>%\n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <\u001b]8;;https://github.com/tidymodels/yardstick/issues\u0007https://github.com/tidymodels/yardstick/issues\u001b]8;;\u0007>.\n\n\n\n\npredict(nausea_fit,nausea_test)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 Yes        \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 Yes        \n10 Yes        \n# … with 173 more rows\n\nnausea_aug_test=augment(nausea_fit,nausea_test)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nnausea_aug_test%>%\n  roc_curve(truth=Nausea,.pred_No)%>%\n  autoplot()\n\n\n\nnausea_aug_train%>%\n  roc_auc(truth=Nausea,.pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.789\n\npredict(nausea_fit,nausea_test)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 Yes        \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 Yes        \n10 Yes        \n# … with 173 more rows\n\nnausea_aug_test=augment(nausea_fit,nausea_test)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nnausea_aug_test%>%\n  roc_auc(truth=Nausea,.pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.723\n\n\n\n\nNew Workflow for Alternative Model\nNow we will make a new workflow for an alternative model which uses the main predictor only (RunnyNose)\n\nset.seed(234)\nnausea_rec2=recipe(Nausea~RunnyNose,data=nausea_train)\nlr_mod=logistic_reg()%>%\n  set_engine(\"glm\")\nnausea_workflow2=workflow()%>%\n  add_model(lr_mod)%>%\n  add_recipe(nausea_rec2)\nnausea_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\nnausea_fit2=nausea_workflow2%>%\n  fit(data=nausea_train)\nnausea_fit2%>%\n  extract_fit_parsnip()%>%\n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -0.629       0.165   -3.80   0.000145\n2 RunnyNoseYes  0.00843     0.197    0.0428 0.966   \n\n\nSimilar to how we did with the first model, let’s use a tidymodels recipe to create a workflow that fits a logistic model for training data and testing data.\n\npredict(nausea_fit2,nausea_train)\n\n# A tibble: 547 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# … with 537 more rows\n\nnausea_aug_train2=augment(nausea_fit2,nausea_train)\nnausea_aug_train2%>%\n  roc_curve(truth=Nausea,.pred_No)%>%\n  autoplot()\n\n\n\npredict(nausea_fit2,nausea_test)\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# … with 173 more rows\n\nnausea_aug_test2=augment(nausea_fit2,nausea_test)\nnausea_aug_test2%>%\n  roc_curve(truth=Nausea,.pred_No)%>%\n  autoplot()\n\n\n\nnausea_aug_train2%>%\n  roc_auc(truth=Nausea,.pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.501\n\npredict(nausea_fit2,nausea_test)\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# … with 173 more rows\n\nnausea_aug_test2=augment(nausea_fit2,nausea_test)\nnausea_aug_test2%>%\n  roc_auc(truth=Nausea,.pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.518\n\n\nIt looks like the model built and trained based on all predictors has a higher ROC_AUC than the alternative model.\n#************THIS SECTION ADDED BY SHIWANI SAPKOTA****************\n\n\nCreating sets for training and testing\n\n# Setting seed\nset.seed(123)\n\n# Putting 3/4 of the data into the training set\ndata_split_part2 <- initial_split(mydata, prop = 3/4)\n\n# Creating data frames for the two sets\ntrain_data_part2 <- training(data_split_part2)\ntest_data_part2  <- testing(data_split_part2)\n\nFULL MODEL: USING BODY TEMPERATURE CONTINUOUS OUTCOME OF INTEREST AND ALL OTHER VARIABLES AS PREDICTORS\n\n# Using Body Temperature as a continuous outcome of interest and all other variables as predictors\nflu_module10_rec_part2 <- recipe(BodyTemp ~ ., data = train_data_part2)\n\n# Fitting the linear model\nflu_module10_mod_part2 <- linear_reg() %>% \n                    set_engine(\"lm\")\n\n# Modelling workflow for pairing model and recipe \nflu_module10_wflow_part2 <- workflow() %>% \n  add_model(flu_module10_mod_part2) %>% \n  add_recipe(flu_module10_rec_part2)\nflu_module10_wflow_part2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n# Using the resulting predictors for preparing recipe and training the model\nflu_module10_fit_part2 <- \n flu_module10_wflow_part2 %>% \n  fit(data = train_data_part2)\n\n# Pulling the fitted model object and using tidy() function for getting a tidy tibble of model coefficients\nflu_module10_fit_part2 %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic   p.value\n   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)           98.2        0.351   279.    0        \n 2 SwollenLymphNodesYes  -0.104      0.108    -0.964 0.336    \n 3 ChestCongestionYes     0.0354     0.114     0.309 0.758    \n 4 ChillsSweatsYes        0.288      0.151     1.90  0.0576   \n 5 NasalCongestionYes    -0.199      0.133    -1.49  0.136    \n 6 CoughYNYes             0.497      0.339     1.47  0.143    \n 7 SneezeYes             -0.390      0.117    -3.34  0.000892 \n 8 FatigueYes             0.373      0.187     1.99  0.0468   \n 9 SubjectiveFeverYes     0.518      0.123     4.22  0.0000287\n10 HeadacheYes           -0.0510     0.151    -0.339 0.735    \n# … with 28 more rows\n\n\nFULL MODEL: USING TRAINED WORKFLOW TO PREDICT\n\n# Using the trained workflow (flu_module10_fit_part2) to predict with the unseen test data\npredict(flu_module10_fit_part2, test_data_part2)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  99.5\n 2  98.4\n 3  99.0\n 4  98.7\n 5  98.5\n 6  99.0\n 7  99.5\n 8  99.7\n 9  98.9\n10  99.6\n# … with 173 more rows\n\n# Using argument() with the model plus test data for saving them together\nflu_module10_aug_part2 <- \n  augment(flu_module10_fit_part2, test_data_part2)\n  \nflu_module10_aug_part2 %>% select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.5\n 2    101.   98.4\n 3     98.8  99.0\n 4     98.5  98.7\n 5     98.1  98.5\n 6     98.4  99.0\n 7     99.5  99.5\n 8     98.8  99.7\n 9    102.   98.9\n10     99.7  99.6\n# … with 173 more rows\n\n# Model fitting using RMSE\nflu_module10_aug_part2 %>% yardstick::rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.15\n\n# Model fitting using R^2\nflu_module10_aug_part2 %>% yardstick::rsq(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard      0.0234\n\n\nALTERNATIVE MODEL: USING BODY TEMPERATURE AS CONTINUOUS OUTCOME OF INTEREST AND RUNNY NOSE AS THE MAIN PREDICTOR\n\n# Using Body Temperature as a continuous outcome of interest and Runny Nose as the main predictor\nflu_module10_rec_part2a <- recipe(BodyTemp ~ RunnyNose, data = train_data_part2)\n\n# Fitting the linear model\nflu_module10_mod_part2a <- linear_reg() %>% \n                    set_engine(\"lm\")\n\n# Modelling workflow for pairing model and recipe \nflu_module10_wflow_part2a <- workflow() %>% \n  add_model(flu_module10_mod_part2a) %>% \n  add_recipe(flu_module10_rec_part2a)\nflu_module10_wflow_part2a\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n# Using the resulting predictors for preparing recipe and training the model\nflu_module10_fit_part2a <- \n flu_module10_wflow_part2a %>% \n  fit(data = train_data_part2)\n\n# Pulling the fitted model object and using tidy() function for getting a tidy tibble of model coefficients\nflu_module10_fit_part2a %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.2      0.0969   1024.   0      \n2 RunnyNoseYes   -0.362    0.115      -3.16 0.00168\n\n\nALTERNATIVE MODEL: USING TRAINED WORKFLOW TO PREDICT\n\n# Using the trained workflow (flu_module10_fit_part2a) to predict with the unseen test data\npredict(flu_module10_fit_part2a, test_data_part2)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  99.2\n 2  98.9\n 3  98.9\n 4  98.9\n 5  98.9\n 6  98.9\n 7  99.2\n 8  99.2\n 9  99.2\n10  99.2\n# … with 173 more rows\n\n# Using argument() with the model plus test data for saving them together\nflu_module10_aug_part2a <- \n  augment(flu_module10_fit_part2a, test_data_part2)\n  \nflu_module10_aug_part2a %>%\n  select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.2\n 2    101.   98.9\n 3     98.8  98.9\n 4     98.5  98.9\n 5     98.1  98.9\n 6     98.4  98.9\n 7     99.5  99.2\n 8     98.8  99.2\n 9    102.   99.2\n10     99.7  99.2\n# … with 173 more rows\n\n# Model fitting using RMSE\nflu_module10_aug_part2a %>% yardstick::rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.12\n\n# Model fitting using R^2\nflu_module10_aug_part2a %>% yardstick::rsq(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard     0.00146\n\n\nFrom the above results, it seems both models performed poorly. The full model (using Body Temperature as the main continuous outcome of interest and all variables as predictors) with RMSE 1.15 performed little better compared to the alternative model (using Body Temperature as the main continuous outcome of interest and Runny Nose as the main predictor) with RMSE 1.12."
  }
]