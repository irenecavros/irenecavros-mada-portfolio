---
title: "Tidy Tuesday Exercise"
output: 
  html_document:
    toc: FALSE
---

#### Introduction

The data for this week's Tidy Tuesday exercise comes from [The Humane League's US Egg Production dataset](https://thehumaneleague.org/article/E008R01-us-egg-production-data) by [Samara Mendez](https://samaramendez.github.io/). Dataset and code is available for this project on OSF at [US Egg Production Data Set](https://osf.io/z2gxn/).

This dataset tracks the supply of cage-free eggs in the United States from December 2007 to February 2021. For TidyTuesday we've used data through February 2021, but the full dataset, with data through the present, is available in the [OSF project](https://osf.io/z2gxn/).

## 1) Load, wrangle and explore the data. By now you know this is an iterative procedure, so it's ok to have these parts of the process/code intertwined.

#### Load packages

```{r}
library('tidyverse')
library('skimr')
library('tidymodels')
library('rpart')
library('rpart.plot')
library('glmnet')
library('ranger')
library('vip')
library('janitor')
library('bonsai')
library('lightgbm')
```

#### Get the data

```{r}
library(tidytuesdayR) #read in `tidytuesdayR`
tuesdata <- tidytuesdayR::tt_load('2023-04-11')
tuesdata <- tidytuesdayR::tt_load(2023, week = 15)
#load in readme and datasets for this week

summary(tuesdata)
```

```{r}
eggproduction <- tuesdata$'egg-production'
cagefreepercentages <- tuesdata$'cage-free-percentages'
```

#### Initial data exploration

Summary stats for `eggproduction`

```{r}
tibble(eggproduction) 
summary(eggproduction)
skim(eggproduction)
```

Simple plots for `eggproduction`

```{r}
# plot number of eggs produced over time
eggtimeplot <- ggplot(eggproduction)+
  geom_point(aes(observed_month, n_eggs, color = prod_process))+ #color the data by production process
  labs(title = "Number of eggs produced by date", x = "Date", y = "Number of eggs produced")
eggtimeplot
```

Let's put this on a log scale so the data is more interpretable

```{r}
eggtimeplot2 <- ggplot(eggproduction, aes(observed_month, log(n_eggs)))+ #log scale the eggs for greater interpretability
  geom_point(aes(color = prod_process))+ 
  labs(title = "Number of eggs produced by date", x = "Date", y = "Log number of eggs produced")
eggtimeplot2
```

Summary stats for `cagefreepercentages`

```{r}
tibble(cagefreepercentages) 
summary(cagefreepercentages)
skim(cagefreepercentages)
```

Simple plots for `cagefreepercentages`

We can start with looking at cage free eggs

```{r}
ggplot(cagefreepercentages, aes(x = observed_month, y = percent_eggs)) +
  geom_area() +
  labs(title = "Proportion of eggs produced which are cage-free over time", x= "Year", y=   "Percentage")
```

Now let's do the same for actual hens

```{r}
ggplot(cagefreepercentages, aes(x = observed_month, y = percent_hens)) +
  geom_area() +
  labs(title = "Proportion of hens producing eggs which are cage-free over time", x= "Year", y=   "Percentage")
```

**Data wrangling**

Merging all the data by date

```{r}
completedata <- inner_join(cagefreepercentages, eggproduction, by="observed_month")
tibble(completedata)
```

Let's remove any extraneous variables for our purposes. In this case we can remove the source variable

```{r}
cleandata <- eggproduction %>%
  select(!c(source))

tibble(cleandata)
```

```{r}
#adding new variable about eggs per hen
finaldata <- cleandata %>% mutate(egg_production = n_eggs/n_hens)
summary(finaldata)
```

```{r}
ggplot() +
  geom_line(data = finaldata, aes(observed_month, log(n_hens), color = prod_process)) +
  ggtitle("Number of organic vs. non-organic hens over time") +
labs(x = "Year", y = "Number of hens", color = "Production process" )
```

The `all` category is a bit confusing but we can deal with that a bit later.

Setting up variables/factors

```{r}
finaldata$prod_process <- as.factor(finaldata$prod_process)
finaldata$prod_type <- as.factor(finaldata$prod_type)

finaldata$prod_process <- factor(finaldata$prod_process, levels = c("cage-free (non-organic)", "cage-free (organic)", "all"), ordered = TRUE)

finaldata$prod_type <- factor(finaldata$prod_type, levels = c("hatching eggs", "table eggs"), ordered=TRUE)
```

## 2) Once you understand the data sufficiently, formulate a question/hypothesis. This will determine your outcome of interest and, if applicable, main predictor(s) of interest. 

Research question of interest: are more eggs produced per hen in cage-free non-organic facilities compared to cage-free organic facilities?

Outcome of interest: egg production

Predictor: production process

**Final wrangling based on research question**

## 3) Then split into train/test.

**Splitting the data**

```{r}
#fix random numbers
set.seed(123) 

#split data into 70% training, 30% testing
split <- initial_split(finaldata, prop = 7/10, strata = n_eggs) 

# Create data frames for the two sets:
train_data <- training(split)
test_data  <- testing(split)
```

**Null model**

Build the null model

```{r}
#setting up null model
nullmod <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression")

#setting up the null recipe
null_recipe <- recipe(egg_production ~ ., data = train_data) %>%
  step_date(observed_month) %>% #changes date column into nominal
  step_rm(observed_month) %>% #removes original date column
  step_dummy(all_nominal_predictors()) #creates dummy variables for predictors

#setting up the null workflow
null_workflow <- workflow() %>%
  add_model(nullmod) %>%
  add_recipe(null_recipe)

#fit null model to training data
nullmod_fit <- null_workflow %>%
  fit(data = train_data)
```

Now we calculate the `rmse` since it is a regression with a continuous outcome

```{r}
#make predictions on TRAINING data using null model
null_predtrain <- augment(nullmod_fit, train_data) %>% 
  rmse(truth = egg_production, .pred) %>% 
  mutate(model = "Null")
null_predtrain
```

Apply to testing data

```{r}
#make predictions on TESTING data using null model
null_predtest <-augment(nullmod_fit, test_data) %>% 
  rmse(egg_production, .pred) %>% 
  mutate(model = "Null")
null_predtest
```

Compared to the training data, the testing data performed slightly better since the test RMSE is slightly lower than the train RMSE.

## 4) Fit at least 4 different ML models to the data using the `tidymodels` framework we practiced. Use the CV approach for model training/fitting. Explore the quality of each model by looking at performance, residuals, uncertainty, etc. All of this should still be evaluated using the training/CV data. 

Setting up for all our models

```{r}
set.seed(456) #for reproducibility
folds <- vfold_cv(train_data, v = 5) #CV folds

#basic recipe used in all models
egg_recipe <- recipe(egg_production ~., data = train_data) %>% 
  step_date(observed_month) %>% #change date column into nominal
  step_rm(observed_month) %>% #remove original date column
  step_dummy(all_nominal_predictors()) #create dummy variables for predictors
```

**Model #1 Decision Tree**

Set up model and workflow

```{r}
#set decision tree model regression
dt_model <- decision_tree(
  min_n = tune(),
  tree_depth = tune(),
  cost_complexity = tune()
) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

#create machine learning workflow
dt_workflow <- workflow() %>% 
  add_model(dt_model) %>% 
  add_recipe(egg_recipe)
```

Tuning grid specification

```{r}
#set up tuning grid
dt_grid <- grid_regular(min_n(),
                          tree_depth(),
                          cost_complexity())

set.seed(456)
dt_res <- dt_workflow %>%
  tune_grid(
    resamples = folds, #CV folds
    grid = dt_grid,
    control = control_grid(save_pred = TRUE)) 
```

```{r}
dt_res %>% autoplot()
dt_res %>% collect_metrics()
```

Choose the best model

```{r}
best_dt <- dt_res %>% select_best("rmse")
best_dt

#summary of best model to be used for later comparison
compare_dt <- dt_res %>% 
  show_best("rmse", n=1) %>% 
  select(c(.metric, mean, std_err)) %>% 
  mutate(model = "Decision Tree")

#set up workflow
dt_workflow_final <- dt_workflow %>% 
  finalize_workflow(best_dt)

#fit the final model
dt_fit_final<- dt_workflow_final %>% 
  fit(train_data) 

#extract RMSE value
finalfitted_dt <- augment(dt_fit_final, train_data)

finalfitted_dt_rmse <- finalfitted_dt%>% 
  select(egg_production, .pred) %>% 
  rmse(truth = egg_production, .pred)%>% 
  mutate(model = "Decision Tree")
finalfitted_dt_rmse
```

The best performing decision tree model has an RMSE of 0.019, cost complexity value of 3.162278e-06, tree_depth of 15, and a mininum node size of 2.

Lastly let's plot the tree

```{r}
rpart.plot(extract_fit_parsnip(dt_fit_final)$fit)
```

**Model #2 Lasso**

Let's start with model specification

```{r}
lr_model <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")
```

Create a workflow

```{r}
lr_workflow <- workflow() %>% 
  add_model(lr_model) %>% 
  add_recipe(egg_recipe)
```

Setting up a grid of tuned parameters for cross validation

```{r}
lr_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30)) #grid of penalty values to tune
```

Tuning using cross-validation and `tune_grid`

```{r}
set.seed(456)
lr_res <- lr_workflow %>%
  tune_grid(
    resamples = folds,
    grid = lr_grid,
    control = control_grid(save_pred = TRUE))
```

Plot and collect metrics

```{r}
lr_res %>% autoplot()
lr_res %>% collect_metrics()
```

Select the best model

```{r}
#select best model
best_lr <- lr_res %>% select_best("rmse")
best_lr

#summary of best model 
compare_lr <- lr_res %>% 
  show_best("rmse", n=1) %>% 
  select(c(.metric, mean, std_err)) %>% 
  mutate(model = "Linear Regression (LASSO)")

#set up workflow
final_lr_workflow <- lr_workflow %>% 
  finalize_workflow(best_lr)

#fit the final model
final_lr_fit <- final_lr_workflow %>% 
  fit(train_data) 

#get the RMSE
finalfitted_lr <- augment(final_lr_fit, train_data)

finalfitted_lr_rmse <- finalfitted_lr %>%
  select(egg_production, .pred) %>% 
  rmse(truth = egg_production, .pred) %>% 
  mutate(model = "Linear Regression (LASSO)")
finalfitted_lr_rmse
```

The best lasso model has a penalty value of .0117 and an RMSE of .31.

**Model #3 Random Forest**

```{r}
#detect computer cores
cores <- parallel::detectCores()
```

```{r}
#set random forest model
rf_model <- rand_forest(mtry = tune(), #parameter to tune based on engine
                      min_n = tune(), 
                      trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% #number of threads for processing
  set_mode("regression")

#set workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(egg_recipe)
```

Tune parameters using CV folds

```{r}
set.seed(456)
rf_res <- rf_workflow %>% 
  tune_grid(folds,
            grid = 25, 
            control = control_grid(save_pred = TRUE), 
            metrics = metric_set(rmse)) 
```

Plot and collect metrics

```{r}
rf_res %>% autoplot()
rf_res %>% collect_metrics()
```

Select best RF model

```{r}
best_rf <- rf_res %>% select_best("rmse")
best_rf

#summary of best model 
compare_rf <- rf_res %>% 
  show_best("rmse", n=1) %>% 
  select(c(.metric, mean, std_err)) %>% 
  mutate(model = "Random Forest")

#set up workflow
final_rf_workflow <- rf_workflow %>% 
  finalize_workflow(best_rf)

#fit the final model
final_rf_fit <- final_rf_workflow %>% 
  fit(train_data) 

#get the RMSE
finalfitted_rf <- augment(final_rf_fit, train_data)

finalfitted_rf_rmse <- finalfitted_rf %>%
  select(egg_production, .pred) %>% 
  rmse(truth = egg_production, .pred) %>% 
  mutate(model = "Random Forest")
finalfitted_rf_rmse
```

The best random forest model has an RMSE of .137 and a mtry of 21.

**Model #4 Boosted tree model**

```{r}
#set boosted tree regression model
bt_model <- boost_tree(tree_depth = tune(), #tuning parameters from engine specs
                    trees = tune(),
                    min_n = tune()) %>%
  set_engine("lightgbm") %>%
  set_mode("regression")

#create boosted tree workflow
bt_workflow <- workflow() %>% 
  add_model(bt_model) %>% 
  add_recipe(egg_recipe)
```

Tuning grid parameters

```{r}
#set up tuning grid for cross validation
bt_grid <- grid_regular(tree_depth(),
                        trees(),
                        min_n())

#tune parameters using cross-validation folds
set.seed(456)
bt_res <- bt_workflow %>% 
  tune_grid(
    resamples = folds,
    grid = bt_grid,
    control = control_grid(save_pred = TRUE)
  )
```

Plot and collect metrics

```{r}
bt_res %>% autoplot()
bt_res %>% collect_metrics()
```

Selecting best model

```{r}
#select best tree model
best_bt <- bt_res %>% select_best("rmse")
best_bt

#summary of best model
compare_bt <- bt_res %>% 
  show_best("rmse", n=1) %>% 
  select(c(.metric, mean, std_err)) %>% 
  mutate(model = "Boosted Tree")

#set up workflow
btfinal_workflow <- bt_workflow %>% 
  finalize_workflow(best_bt)

#fit the final model
bt_fit_final <- btfinal_workflow %>% 
  fit(train_data) 

#extract RMSE value
finalfitted_bt <- augment(bt_fit_final, train_data)

finalfitted_bt_rmse <- finalfitted_bt %>% 
  select(egg_production, .pred) %>% 
  rmse(truth = egg_production, .pred)%>% 
  mutate(model = "Boosted Tree")
finalfitted_bt_rmse
```

The best boosted tree model has an RMSE of 0.143*,* 2000 trees, minimal node size of 2, and a tree depth of 1.

## 5) Based on the model evaluations, decide on one model you think is overall best. Explain why. It doesn't have to be the model with the best performance. You make the choice, just explain why you picked the one you picked.

```{r}
compare <- bind_rows(compare_lr, compare_rf, compare_dt, compare_bt)
compare
```

Compare RMSEs

```{r}
comparefit <- bind_rows(null_predtrain, finalfitted_lr_rmse, finalfitted_rf_rmse, finalfitted_dt_rmse, finalfitted_bt_rmse)
comparefit
```

All models performed better than the null model. Based on the RMSE values, the linear regression performed the worst of all models and the decision tree model, boosted tree model, and random forest model all are neck and neck in terms of RMSE, with the lowest RMSE being produced via decision tree. So we will move forward with the decision tree model.

## 6) As a final, somewhat honest assessment of the quality of the model you chose, evaluate it (performance, residuals, uncertainty, etc.) on the **test data**. This is the only time you are allowed to touch the test data, and only once. Report model performance on the test data.

```{r}
#fit final model to test data
dt_test_fit <- dt_workflow_final %>% 
  last_fit(split) #uses best model on test data

#metrics from final model
dt_test_fit %>% collect_metrics()
```

Compare with null model

```{r}
null_predtest
```

The decision tree model performed better than the null model on the test data. The model did perform worse on test data compared to training data, but this is okay.

## 7) Summarize everything you did and found in a discussion. Of course, your Rmd file should contain commentary/documentation on everything you do for each step.

**Discussion**

In this exercise, I used cross-validation to tune 4 models attempting to predict `egg_production` from three predictors (`prod_type`, `prod_process`, and `observed_month`). The four models (decision tree, random forest, LASSO, and boosted tree model) all performed better than the null model. Due to the small residual values, low RMSE value, I choose to move forward with the decision tree model for the final test data fit. This final fit had a RMSE of 0.396 and an R\^2 of 0.968, which led me to believe the model is performing well on the new data.
